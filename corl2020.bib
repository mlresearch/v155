@Proceedings{ICE-2013,
    booktitle = {Proceedings of the 2020 Conference on Robot Learning},
    name = {Conference on Robot Learning},
    shortname = {CoRL},
    editor = {Jens Kober and Fabio Ramos and Claire Tomlin},
    volume = {155},
    year = {2020},
    start = {2020-11-16},
    end = {2020-11-18},
    published = {2021-10-04},
    conference_url = {https://sites.google.com/robot-learning.org/corl2020},
    address = {virtual}
}

@InProceedings{huang20,    
    title  =  {Learning a Decision Module by Imitating Driver's Control Behaviors},    
    author  =  {Huang, Junning and Xie, Sirui and Sun, Jiankai and Ma, Qiurui and Liu, Chunxiao and Lin, Dahua and Zhou, Bolei},    
    pages = {1--10},     
    abstract  =  {Autonomous driving systems have a pipeline of perception, decision, planning, and control. The decision module processes information from the perception module and directs the execution of downstream planning and control modules. On the other hand, the recent success of deep learning suggests that this pipeline could be replaced by end-to-end neural control policies, however, safety cannot be well guaranteed for the data-driven neural networks. In this work, we propose a hybrid framework to learn neural decisions in the classical modular pipeline through end-to-end imitation learning.  This hybrid framework can preserve the merits of the classical pipeline such as the strict enforcement of physical and logical constraints while learning complex driving decisions from data. To circumvent the ambiguous annotation of human driving decisions, our method learns high-level driving decisions by imitating low-level control behaviors. We show in the simulation experiments that our modular driving agent can generalize its driving decision and control to various complex scenarios where the rule-based programs fail. It can also generate smoother and safer driving trajectories than end-to-end neural policies. Demo and code are available at https://decisionforce.github.io/modulardecision/.},    
    paperid  =  {4}    
}    

@InProceedings{weng20,    
    title  =  {Inverting the Pose Forecasting Pipeline with SPF2: Sequential Pointcloud Forecasting for Sequential Pose Forecasting},    
    author  =  {Weng, Xinshuo and Wang, Jianren and Levine, Sergey and Kitani, Kris and Rhinehart, Nicholas},    
    pages  =  {11--20},     
    abstract  =  {Many autonomous systems forecast aspects of the future in order to aid decision-making. For example, self-driving vehicles and robotic manipulation systems often forecast future object poses by first detecting and tracking objects. However, this detect-then-forecast pipeline is expensive to scale, as pose forecasting algorithms typically require labeled sequences of object poses, which are costly to obtain in 3D space. Can we scale performance without requiring additional labels? We hypothesize yes, and propose inverting the detect-then-forecast pipeline. Instead of detecting, tracking and then forecasting the objects, we propose to first forecast 3D sensor data (e.g., point clouds with $100$k points) and then detect/track objects on the predicted point cloud sequences to obtain future poses, i.e., a forecast-then-detect pipeline. This inversion makes it less expensive to scale pose forecasting, as the sensor data forecasting task requires no labels. Part of this work's focus is on the challenging first step -- Sequential Pointcloud Forecasting (SPF), for which we also propose an effective approach, SPFNet. To compare our forecast-then-detect pipeline relative to the detect-then-forecast pipeline, we propose an evaluation procedure and two metrics. Through experiments on a robotic manipulation dataset and two driving datasets, we show that SPFNet is effective for the SPF task, our forecast-then-detect pipeline outperforms the detect-then-forecast approaches to which we compared, and that pose forecasting performance improves with the addition of unlabeled data. Our project website is http://www.xinshuoweng.com/projects/SPF2},    
    paperid  =  {6}    
}    

@InProceedings{sun20a,    
    title  =  {Neuro-Symbolic Program Search for Autonomous Driving Decision Module Design},    
    author  =  {Sun, Jiankai and Sun, Hao and Han, Tian and Zhou, Bolei},    
    pages  =  {21--30},     
    abstract  =  {As a promising topic in cognitive robotics, neuro-symbolic modeling integrates symbolic reasoning and neural representation altogether. However, previous neuro-symbolic models usually wire their structures and the connections manually, making the underlying parameters sub-optimal. In this work, we propose the Neuro-Symbolic Program Search (NSPS) to improve the autonomous driving system design. NSPS is a novel automated search method that synthesizes the Neuro-Symbolic Programs. It can produce robust and expressive Neuro-Symbolic Programs and automatically tune the hyper-parameters. We validate NSPS in the CARLA driving simulation environment. The resulting Neuro-Symbolic Decision Programs successfully handle multiple traffic scenarios. Compared with previous neural-network-based driving and rule-based methods, our neuro-symbolic driving pipeline achieves more stable and safer behaviors in complex driving scenarios while maintaining an interpretable symbolic decision-making process.},    
    paperid  =  {16}    
}    

@InProceedings{shah20,    
    title  =  {LiRaNet: End-to-End Trajectory Prediction using Spatio-Temporal Radar Fusion},    
    author  =  {Shah, Meet and Huang, Zhiling and Laddha, Ankit and Langford, Matthew and Barber, Blake and zhang, sida and Vallespi-Gonzalez, Carlos and Urtasun, Raquel},    
    pages  =  {31--48},     
    abstract  =  {In this paper, we present LiRaNet, a novel end-to-end trajectory prediction method which utilizes radar sensor information along with widely used lidar and HD maps. Automotive radar provides rich, complementary information, allowing for longer range vehicle detection as well as instantaneous radial velocity measurements. However, there are factors that make the fusion of lidar and radar information challenging, such as the relatively low angular resolution of radar measurements, their sparsity and the lack of exact time synchronization with lidar. To overcome these challenges, we propose an efficient spatio-temporal radar feature extraction scheme which achieves state-of-the-art performance on multiple large-scale datasets. Further, by incorporating radar information, we show a 52\% reduction in prediction error for objects with high acceleration and a 16\% reduction in prediction error for objects at longer range.},    
    paperid  =  {17}    
}    

@InProceedings{choi20,    
    title  =  {DROGON: A Trajectory Prediction Model based on Intention-Conditioned Behavior Reasoning},    
    author  =  {Choi, Chiho and Malla, Srikanth and Patil, Abhishek and Choi, Joon Hee},    
    pages  =  {49--63},     
    abstract  =  {We propose a Deep RObust Goal-Oriented trajectory prediction Network (DROGON) for accurate vehicle trajectory prediction by considering behavioral intentions of vehicles in traffic scenes. Our main insight is that the behavior (i.e., motion) of drivers can be reasoned from their high level possible goals (i.e., intention) on the road. To succeed in such behavior reasoning, we build a conditional prediction model to forecast goal-oriented trajectories with the following stages: (i) relational inference where we encode relational interactions of vehicles using the perceptual context; (ii) intention estimation to compute the probability distributions of intentional goals based on the inferred relations; and (iii) behavior reasoning where we reason about the behaviors of vehicles as trajectories conditioned on the intentions. To this end, we extend the proposed framework to the pedestrian trajectory prediction task, showing the potential applicability toward general trajectory prediction.},    
    paperid  =  {18}    
}    

@InProceedings{chitnis20,    
    title  =  {CAMPs: Learning Context-Specific Abstractions for Efficient Planning in Factored MDPs},    
    author  =  {Chitnis, Rohan and Silver, Tom and Kim, Beomjoon and Kaelbling, Leslie and Lozano-Perez, Tomas},    
    pages  =  {64--79},     
    abstract  =  {Meta-planning, or learning to guide planning from experience, is a promising approach to improving the computational cost of planning. A general meta-planning strategy is to learn to impose constraints on the states considered and actions taken by the agent. We observe that (1) imposing a constraint can induce context-specific independences that render some aspects of the domain irrelevant, and (2) an agent can take advantage of this fact by imposing constraints on its own behavior. These observations lead us to propose the context-specific abstract Markov decision process (CAMP), an abstraction of a factored MDP that affords efficient planning. We then describe how to learn constraints to impose so the CAMP optimizes a trade-off between rewards and computational cost. Our experiments consider five planners across four domains, including robotic navigation among movable obstacles (NAMO), robotic task and motion planning for sequential manipulation, and classical planning. We find planning with learned CAMPs to consistently outperform baselines, including Stilman's NAMO-specific algorithm. Video: https://youtu.be/wTXt6djcAd4 Code: https://git.io/JTnf6},    
    paperid  =  {23}    
}    

@InProceedings{jena20,    
    title  =  {Augmenting GAIL with BC for sample efficient imitation learning},    
    author  =  {Jena, Rohit and Liu, Changliu and Sycara, Katia},    
    pages  =  {80--90},     
    abstract  =  {Imitation learning is the problem of recovering an expert policy without access to a reward signal. Behavior cloning and GAIL are two widely used methods for performing imitation learning. Behavior cloning converges in a few iterations, but doesn't achieve peak performance due to its inherent iid assumption about the state-action distribution. GAIL addresses the issue by accounting for the temporal dependencies when performing a state distribution matching between the agent and the expert. Although GAIL is sample efficient in the number of expert trajectories required, it is still not very sample efficient in terms of the environment interactions needed for convergence of the policy. Given the complementary benefits of both methods, we present a simple and elegant method to combine both methods to enable stable and sample efficient learning. Our algorithm is very simple to implement and integrates with different policy gradient algorithms. We demonstrate the effectiveness of the algorithm in low dimensional control tasks, gridworlds and in high dimensional image-based tasks.},    
    paperid  =  {25}    
}    

@InProceedings{jain20a,    
    title  =  {From pixels to legs: Hierarchical learning of quadruped locomotion},    
    author  =  {Jain, Deepali and Caluwaerts, Ken and Iscen, Atil},    
    pages  =  {91--102},     
    abstract  =  {Legged robots navigating crowded scenes and complex terrains in the real world are required to execute dynamic leg movements while processing visual input for obstacle avoidance and path planning. We show that a quadruped robot can acquire both of these skills by means of hierarchical reinforcement learning (HRL). By virtue of their hierarchical structure, our policies learn to implicitly break down this joint problem by concurrently learning High Level (HL) and Low Level (LL) neural network policies. These two levels are connected by a low dimensional hidden layer, which we call latent command. HL receives a first-person camera view, whereas LL receives the latent command from HL and the robot's on-board sensors to control its actuators. We train policies to walk in two different environments: a curved cliff and a maze. We show that hierarchical policies can concurrently learn to locomote and navigate in these environments, and show they are more efficient than non-hierarchical neural network policies. This architecture also allows for knowledge reuse across tasks. LL networks trained on one task can be transferred to a new task in a new environment. Finally HL, which processes camera images, can be evaluated at much lower and varying frequencies compared to LL, thus reducing computation times and bandwidth requirements.},    
    paperid  =  {28}    
}    

@InProceedings{ha20a,    
    title  =  {Learning a Decentralized Multi-Arm Motion Planner},    
    author  =  {Ha, Huy and Xu, Jingxi and Song, Shuran},    
    pages  =  {103--114},     
    abstract  =  {We present a closed-loop multi-arm motion planner that is scalable and flexible with team size. Traditional multi-arm robotic systems have relied on centralized motion planners, whose run times often scale exponentially with team size, and thus, fail to handle dynamic environments with open-loop control. In this paper, we tackle this problem with multi-agent reinforcement learning, where a shared policy network is trained to control each individual robot arm to reach its target end-effector pose given observations of its workspace state and target end-effector pose. The policy is trained using Soft Actor-Critic with expert demonstrations from a sampling-based motion planning algorithm (i.e., BiRRT). By leveraging classical planning algorithms, we can improve the learning efficiency of the reinforcement learning algorithm while retaining the fast inference time of neural networks. The resulting policy scales sub-linearly and can be deployed on multi-arm systems with variable team sizes. Thanks to the closed-loop and decentralized formulation, our approach generalizes to 5-10 multiarm systems and dynamic moving targets (>90\% success rate for a 10-arm system), despite being trained on only 1-4 arm planning tasks with static targets.},    
    paperid  =  {30}    
}    

@InProceedings{xu20a,    
    title  =  {SelfVoxeLO: Self-supervised LiDAR Odometry with Voxel-based Deep Neural Networks},    
    author  =  {Xu, Yan and Huang, Zhaoyang and Lin, Kwan-Yee and Zhu, Xinge and Shi, Jianping and Bao, Hujun and Zhang, Guofeng and Li, Hongsheng},    
    pages  =  {115--125},     
    abstract  =  {Recent learning-based LiDAR odometry methods have demonstrated their competitiveness. However, most methods still face two substantial challenges: 1) the 2D projection representation of LiDAR data cannot effectively encode 3D structures from the point clouds; 2) the needs for a large amount of labeled data for training limit the application scope of these methods. In this paper, we propose an self-supervised LiDAR odometry method, dubbed SelfVoxeLO, to tackle these two difficulties. Specifically, we propose a 3D convolution network to process the raw LiDAR data directly, which extracts features that better encode the 3D geometric patterns. To suit our network to self-supervised learning, we design several novel loss functions that utilize the inherent properties of LiDAR point clouds. Moreover, an uncertainty-aware mechanism is incorporated in the loss functions to alleviate the interference of moving objects/noises. We evaluate our method's performances on two large-scale datasets, ie, KITTI and Apollo-SouthBay.Our method outperforms state-of-the-art unsupervised methods by 27\%-32\% in terms of translational/rotational errors on the KITTI dataset and also performs well on the Apollo-SouthBay dataset. By including more unlabelled training data, our method can further improve performance comparable to the supervised methods.},    
    paperid  =  {33}    
}    

@InProceedings{xu20b,    
    title  =  {Learning 3D Dynamic Scene Representations for Robot Manipulation},    
    author  =  {Xu, Zhenjia and He, Zhanpeng and Wu, Jiajun and Song, Shuran},    
    pages  =  {126--142},     
    abstract  =  {3D scene representation for robot manipulation should capture three key object properties: permanency - objects that become occluded over time continue to exist; amodal completeness - objects have 3D occupancy, even if only partial observations are available; spatiotemporal continuity - the movement of each object is continuous over space and time. In this paper, we introduce 3D Dynamic Scene Representation (DSR), a 3D volumetric scene representation that simultaneously discovers, tracks, reconstructs objects, and predicts their dynamics while capturing all three properties. We further propose DSR-Net, which learns to aggregate visual observations over multiple interactions to gradually build and refine DSR. Our model achieves state-of-the-art performance in modeling 3D scene dynamics with DSR on both simulated and real data. Combined with model predictive control, DSR-Net enables accurate planning in downstream robotic manipulation tasks such as planar pushing. Code and data are available at dsr-net.cs.columbia.edu.},    
    paperid  =  {35}    
}    

@InProceedings{wang20a,    
    title  =  {CoT-AMFlow: Adaptive Modulation Network with Co-Teaching Strategy for Unsupervised Optical Flow Estimation},    
    author  =  {Wang, Hengli and Fan, Rui and Liu, Ming},    
    pages  =  {143--155},     
    abstract  =  {The interpretation of ego motion and scene change is a fundamental task for mobile robots. Optical flow information can be employed to estimate motion in the surroundings. Recently, unsupervised optical flow estimation has become a research hotspot.  However, unsupervised approaches are often easy to be unreliable on partially occluded or texture-less regions. To deal with this problem, we propose CoT-AMFlow in this paper, an unsupervised optical flow estimation approach. In terms of the network architecture, we develop an adaptive modulation network that employs two novel module types, flow modulation modules (FMMs) and cost volume modulation modules (CMMs), to remove outliers in challenging regions. As for the training paradigm, we adopt a co-teaching strategy, where two networks simultaneously teach each other about challenging regions to further improve accuracy. Experimental results on the MPI Sintel, KITTI Flow and Middlebury Flow benchmarks demonstrate that our CoT-AMFlow outperforms all other state-of-the-art unsupervised approaches, while still running in real time. Our project page is available at https://sites.google.com/view/cot-amflow.},    
    paperid  =  {36}    
}    

@InProceedings{zhao20a,    
    title  =  {SAM: Squeeze-and-Mimic Networks for Conditional Visual Driving Policy Learning},    
    author  =  {Zhao, Albert and He, Tong and Liang, Yitao and Huang, Haibin and Broeck, Guy Van den and Soatto, Stefano},    
    pages  =  {156--175},     
    abstract  =  {We describe a policy learning approach to map visual inputs to driving controls conditioned on turning command that leverages side tasks on semantics and object affordances via a learned representation trained for driving. To learn this representation, we train a squeeze network to drive using annotations for the side task as input. This representation encodes the driving-relevant information associated with the side task while ideally throwing out side task-relevant but driving-irrelevant nuisances. We then train a mimic network to drive using only images as input and use the squeeze network's latent representation to supervise the mimic network via a mimicking loss. Notably, we do not aim to achieve the side task nor to learn features for it; instead, we aim to learn, via the mimicking loss, a representation of the side task annotations directly useful for driving. We test our approach using the CARLA simulator. In addition, we introduce a more challenging but realistic evaluation protocol that considers a run that reaches the destination successful only if it does not violate common traffic rules.},    
    paperid  =  {40}    
}    

@InProceedings{ha20b,    
    title  =  {Fit2Form: 3D Generative Model for Robot Gripper Form Design},    
    author  =  {Ha, Huy and Agrawal, Shubham and Song, Shuran},    
    pages  =  {176--187},     
    abstract  =  {The 3D shape of a robot's end-effector plays a critical role in determining it's functionality and overall performance. Many of today's industrial applications rely on highly customized gripper design for a given task to ensure the system's robustness and accuracy.  However, the process of manual hardware design is both costly and time-consuming, and the quality of the design is also dependent on the engineer's experience and domain expertise, which can easily be out-dated or inaccurate.  The goal of this paper is to use machine learning algorithms to automate this design process and generate task-specific gripper designs that satisfy a set of pre-defined design objectives. We model the design objectives by training a Fitness network to predict their values for a pair of gripper fingers and a grasp object. This Fitness network is then used to provide training supervision to a 3D Generative network that produces a pair of 3D finger geometries for the target grasp object. Our experiments demonstrate that the proposed 3D generative design framework generates parallel jaw gripper finger shapes that achieve more stable and robust grasps as compared to other general-purpose and task-specific gripper design algorithms.},    
    paperid  =  {41}    
}    

@InProceedings{pertsch20,    
    title  =  {Accelerating Reinforcement Learning with Learned Skill Priors},    
    author  =  {Pertsch, Karl and Lee, Youngwoon and Lim, Joseph},    
    pages  =  {188--204},     
    abstract  =  {Intelligent agents rely heavily on prior experience when learning a new task, yet most modern reinforcement learning (RL) approaches learn every task from scratch. One approach for leveraging prior knowledge is to transfer skills learned on prior tasks to the new task. However, as the amount of prior experience increases, the number of transferable skills grows too, making it challenging to explore the full set of available skills during downstream learning. Yet, intuitively, not all skills should be explored with equal probability; for example information about the current state can hint which skills are promising to explore. In this work, we propose to implement this intuition by learning a prior over skills. We propose a deep latent variable model that jointly learns an embedding space of skills and the skill prior from offline agent experience. We then extend common maximum-entropy RL approaches to use skill priors to guide downstream learning. We validate our approach, SPiRL (Skill-Prior RL), on complex navigation and robotic manipulation tasks and show that learned skill priors are essential for effective skill transfer from rich datasets. Videos and code are available at https://clvrai.com/spirl.},    
    paperid  =  {44}    
}    

@InProceedings{xu20c,    
    title  =  {Positive-Unlabeled Reward Learning},    
    author  =  {Xu, Danfei and Denil, Misha},    
    pages  =  {205--219},     
    abstract  =  {Learning reward functions from data is a promising path towards achieving scalable Reinforcement Learning (RL) for robotics. However, a major challenge in training agents from learned reward models is that the agent can learn to exploit errors in the reward model to achieve high reward behaviors that do not correspond to the intended task. These reward delusions can lead to unintended and even dangerous behaviors. On the other hand, adversarial imitation learning frameworks (Ho et al., 2016) tend to suffer the opposite problem, where the discriminator learns to trivially distinguish agent and expert behavior, resulting in reward models that produce low reward signal regardless of the input state. In this paper, we connect these two classes of reward learning methods to positive-unlabeled (PU) learning, and show that by applying a large-scale PU learning algorithm to the reward learning problem, we can address both the reward under- and over-estimation problems simultaneously. Our approach drastically improves both GAIL and supervised reward learning, without any additional assumptions.},    
    paperid  =  {45}    
}    

@InProceedings{jeng20,    
    title  =  {GDN: A Coarse-To-Fine (C2F) Representation for End-To-End 6-DoF Grasp Detection},    
    author  =  {Jeng, Kuang-Yu and Liu, Yueh-Cheng and Liu, Zhe Yu and Wang, Jen-Wei and Chang, Ya-Liang and Su, Hung-Ting and Hsu, Winston},    
    pages  =  {220--231},     
    abstract  =  {We proposed an end-to-end grasp detection network,  Grasp Detection Network (GDN), cooperated with a novel coarse-to-fine (C2F) grasp representation design to detect diverse and accurate 6-DoF grasps based on point clouds.   Compared to previous two-stage approaches which sample and evaluate multiple grasp candidates, our architecture is at least 20 times faster.  It is also 8\% and 40\% more accurate in terms of the success rate in single object scenes and the complete rate in clutter scenes, respectively. Our method shows superior results among settings with different number of views and input points.  Moreover, we propose a new AP-based metric which considers both rotation and transition errors, making it a more comprehensive evaluation tool for grasp detection models.},    
    paperid  =  {47}    
}    

@InProceedings{xiao20,    
    title  =  {Action-based Representation Learning for Autonomous Driving},    
    author  =  {Xiao, Yi and Codevilla, Felipe and Pal, Christopher and Lopez, Antonio},    
    pages  =  {232--246},     
    abstract  =  {Human drivers produce a vast amount of data which could, in principle, be used to improve autonomous driving systems. Unfortunately, seemingly straightforward approaches for creating end-to-end driving models that map sensor data directly into driving actions are problematic in terms of interpretability, and typically have significant difficulty dealing with spurious correlations. Alternatively, we propose to use this kind of action-based driving data for learning representations. Our experiments show that an affordance-based driving model pre-trained with this approach can leverage a relatively small amount of weakly annotated imagery and outperform pure end-to-end driving models, while being more interpretable. Further, we demonstrate how this strategy outperforms previous methods based on learning inverse dynamics models as well as other methods based on heavy human supervision (ImageNet).},    
    paperid  =  {49}    
}    

@InProceedings{zolna20,    
    title  =  {Task-Relevant Adversarial Imitation Learning},    
    author  =  {Zolna, Konrad and Reed, Scott and Novikov, Alexander and Colmenarejo, Sergio G\'{o}mez and Budden, David and Cabi, Serkan and Denil, Misha and Freitas, Nando de and Wang, Ziyu},    
    pages  =  {247--263},     
    abstract  =  {We show that a critical vulnerability in adversarial imitation is the tendency of discriminator networks to learn spurious associations between visual features and expert labels. When the discriminator focuses on task-irrelevant features, it does not provide an informative reward signal, leading to poor task performance. We analyze this problem in detail and propose a solution that outperforms standard Generative Adversarial Imitation Learning (GAIL). Our proposed method, Task-Relevant Adversarial Imitation Learning (TRAIL), uses constrained discriminator optimization to learn informative rewards. In comprehensive experiments, we show that TRAIL can solve challenging robotic manipulation tasks from pixels by imitating human operators without access to any task rewards, and clearly outperforms comparable baseline imitation agents, including those trained via behaviour cloning and conventional GAIL.},    
    paperid  =  {50}    
}    

@InProceedings{zhou20a,    
    title  =  {SMARTS: An Open-Source Scalable Multi-Agent RL Training School for Autonomous Driving},    
    author  =  {Zhou, Ming and Luo, Jun and Villella, Julian and Yang, Yaodong and Rusu, David and Miao, Jiayu and Zhang, Weinan and Alban, Montgomery and FADAKAR, IMAN and Chen, Zheng and Huang, Chongxi and Wen, Ying and Hassanzadeh, Kimia and Graves, Daniel and Zhu, Zhengbang and Ni, Yihan and Nguyen, Nhat and Elsayed, Mohamed and Ammar, Haitham and Cowen-Rivers, Alexander and Ahilan, Sanjeevan and Tian, Zheng and Palenicek, Daniel and Rezaee, Kasra and Yadmellat, Peyman and Shao, Kun and chen, dong and Zhang, Baokuan and Zhang, Hongbo and Hao, Jianye and Liu, Wulong and Wang, Jun},    
    pages  =  {264--285},     
    abstract  =  {Interaction is fundamental in autonomous driving (AD). Despite more than a decade of intensive R&D in AD, how to dynamically interact with diverse road users in various contexts still remains unsolved. Multi-agent learning has recently seen big breakthroughs and has much to offer towards solving realistic interaction in AD. However, to realize this potential we need multi-agent AD simulation of realistic interaction. To break this apparent chicken-and-egg circularity, we built an AD simulation platform called SMARTS (Scalable Multi-Agent Rl Training School), which is designed to accumulate behavior models of road users towards increasingly realistic and diverse interaction that in turn enables deeper and broader multi-agent research on interaction. In this paper, we describe the design goals of SMARTS, explain its key architectural ideas, illustrate its use for multi-agent research through experiments on concrete interaction scenarios, and introduce a set of benchmarks and metrics. As an open-source, industrial-strength platform, the future of SMARTS lies in its growth along with the multi-agent research it enables in the years to come.},    
    paperid  =  {53}    
}    

@InProceedings{wang20b,    
    title  =  {Reconfigurable Voxels: A New Representation for LiDAR-Based Point Clouds},    
    author  =  {Wang, Tai and Zhu, Xinge and Lin, Dahua},    
    pages  =  {286--295},     
    abstract  =  {LiDAR is an important method for autonomous driving systems to sense the environment. The point clouds obtained by LiDAR typically exhibit sparse and irregular distribution, thus posing great challenges to the detection of 3D objects, especially those that are small and distant. To tackle this difficulty, we propose Reconfigurable Voxels, a new approach to constructing representations from 3D point clouds. Specifically, we devise a biased random walk scheme, which adaptively covers each neighborhood with a fixed number of voxels based on the local spatial distribution and produces a representation by integrating the points in the chosen neighbors. We found empirically that this approach effectively improves the stability of voxel features, especially for sparse regions. Experimental results on multiple benchmarks, including nuScenes, Lyft, and KITTI, show that this new representation can remarkably improve the detection performance for small and distant objects, without incurring noticeable overhead cost},    
    paperid  =  {58}    
}    

@InProceedings{petrik20,    
    title  =  {Learning Object Manipulation Skills via Approximate State Estimation from Real Videos},    
    author  =  {Petr\'{i}k, Vladim\'{i}r and Tapaswi, Makarand and Laptev, Ivan and Sivic, Josef},    
    pages  =  {296--312},     
    abstract  =  {Humans are adept at learning new tasks by watching a few instructional videos. On the other hand, robots that learn new actions either require a lot of effort through trial and error, or use expert demonstrations that are challenging to obtain. In this paper, we explore a method that facilitates learning object manipulation skills directly from videos. Leveraging recent advances in 2D visual recognition and differentiable rendering, we develop an optimization based method to estimate a coarse 3D state representation for the hand and the manipulated object(s) without requiring any supervision. We use these trajectories as dense rewards for an agent that learns to mimic them through reinforcement learning. We evaluate our method on simple single- and two-object actions from the Something-Something dataset. Our approach allows an agent to learn actions from single videos, while watching multiple demonstrations makes the policy more robust. We show that policies learned in a simulated environment can be easily transferred to a real robot.},    
    paperid  =  {59}    
}    

@InProceedings{datta20,    
    title  =  {Integrating Egocentric Localization for More Realistic Point-Goal Navigation Agents},    
    author  =  {Datta, Samyak and Maksymets, Oleksandr and Hoffman, Judy and Lee, Stefan and Batra, Dhruv and Parikh, Devi},    
    pages  =  {313--328},     
    abstract  =  {Recent work has presented embodied agents that can navigate to point-goal targets in novel indoor environments with near-perfect accuracy. However, these agents are equipped with idealized sensors for localization and take deterministic actions. This setting is practically sterile by comparison to the dirty reality of noisy sensors and actuations in the real world -- wheels can slip, motion sensors have error, actuations can rebound. In this work, we take a step towards this noisy reality, developing point-goal navigation agents that rely on visual estimates of egomotion under noisy action dynamics. We find these agents outperform naive adaptions of current point-goal agents to this setting as well as those incorporating classic localization baselines. Further, our model conceptually divides learning agent dynamics or odometry (where am I?) from task-specific navigation policy (where do I want to go?). This enables a seamless adaption to changing dynamics (a different robot or floor type) by simply re-calibrating the visual odometry model -- circumventing the expense of re-training of the navigation policy. Our agent was the runner-up in the PointNav track of CVPR 2020 Habitat Challenge.},    
    paperid  =  {61}    
}    

@InProceedings{buhet20,    
    title  =  {PLOP: Probabilistic Polynomial Objects trajectory Prediction for autonomous driving},    
    author  =  {Buhet, Thibault and Wirbel, Emilie and Bursuc, Andrei and Perrotton, Xavier},    
    pages  =  {329--338},     
    abstract  =  {To navigate safely in urban environments, an autonomous vehicle (ego vehicle) must understand and anticipate its surroundings, in particular the behavior and intents of other road users (neighbors). Most of the times, multiple decision choices are acceptable for all road users (e.g., turn right or left, or different ways of avoiding an obstacle), leading to a highly uncertain and multi-modal decision space. We focus here on predicting multiple feasible future trajectories for both ego vehicle and neighbors through a probabilistic framework. We rely on a conditional imitation learning algorithm, conditioned by a navigation command for the ego vehicle (e.g., ``turn right''). Our model processes ego vehicle front-facing camera images and bird-eye view grid, computed from Lidar point clouds, with detections of past and present objects, in order to generate multiple trajectories for both ego vehicle and its neighbors. Our approach is computationally efficient and relies only on on-board sensors. We evaluate our method offline on the publicly available dataset nuScenes, achieving state-of-the-art performance, investigate the impact of our architecture choices on online simulated experiments and show preliminary insights for real vehicle control.},    
    paperid  =  {65}    
}    

@InProceedings{schmeckpeper20,    
    title  =  {Reinforcement Learning with Videos: Combining Offline Observations with Interaction},    
    author  =  {Schmeckpeper, Karl and Rybkin, Oleh and Daniilidis, Kostas and Levine, Sergey and Finn, Chelsea},    
    pages  =  {339--354},     
    abstract  =  { Reinforcement learning is a powerful framework for robots to acquire skills from experience, but often requires a substantial amount of online data collection. As a result, it is difficult to collect sufficiently diverse experiences that are needed for robots to generalize broadly. Videos of humans, on the other hand, are a readily available source of broad and interesting experiences. In this paper, we consider the question: can we perform reinforcement learning directly on experience collected by humans? This problem is particularly difficult, as such videos are not annotated with actions and exhibit substantial visual domain shift relative to the robot's embodiment. To address these challenges, we propose a framework for reinforcement learning with videos (RLV). RLV learns a policy and value function using experience collected by humans in combination with data collected by robots. In our experiments, we find that RLV is able to leverage such videos to learn challenging vision-based skills with less than half as many samples as RL methods that learn from scratch.},    
    paperid  =  {66}    
}    

@InProceedings{strudel20,    
    title  =  {Learning Obstacle Representations for Neural Motion Planning},    
    author  =  {STRUDEL, Robin and Pinel, Ricardo Garcia and Carpentier, Justin and Laumond, Jean-Paul and Laptev, Ivan and Schmid, Cordelia},    
    pages  =  {355--364},     
    abstract  =  {Motion planning and obstacle avoidance is a key challenge in robotics applications. While previous work succeeds to provide excellent solutions for known environments, sensor-based motion planning in new and dynamic environments remains to be  difficult. In this work we address sensor-based motion planning from a learning perspective. Motivated by recent advances in visual recognition, we argue the importance of learning appropriate representations for motion planning. We propose a new obstacle representation based on the PointNet architecture and learn it jointly with policies for obstacle avoidance. We experimentally evaluate our approach for rigid body motion planning in challenging environments and demonstrate significant improvements of the state of the art in terms of accuracy and efficiency.},    
    paperid  =  {70}    
}    

@InProceedings{wang20c,    
    title  =  {CLOUD: Contrastive Learning of Unsupervised Dynamics},    
    author  =  {Wang, Jianren and Lu, Yujie and Zhao, Hang},    
    pages  =  {365--376},     
    abstract  =  {Developing agents that can perform complex control tasks from high dimensional observations such as pixels is challenging due to difficulties in learning dynamics efficiently. In this work, we propose to learn forward and inverse dynamics in a fully unsupervised manner via contrastive estimation. Specifically, we train a forward dynamics model and an inverse dynamics model in the feature space of states and actions with data collected from random exploration. Unlike most existing deterministic models, our energy-based model takes into account the stochastic nature of agent-environment interactions. We demonstrate the efficacy of our approach across a variety of tasks including goal-directed planning and imitation from observations.},    
    paperid  =  {74}    
}    

@InProceedings{danielczuk20,    
    title  =  {Exploratory Grasping: Asymptotically Optimal Algorithms for Grasping Challenging Polyhedral Objects},    
    author  =  {Danielczuk, Michael and Balakrishna, Ashwin and Brown, Daniel and Goldberg, Ken},    
    pages  =  {377--393},     
    abstract  =  {There has been significant recent work on data-driven algorithms for learning general-purpose grasping policies.  However, these policies can consistently fail to grasp challenging objects which are significantly out of the distribution of objects in the training data or which have very few high quality grasps. Motivated by such objects, we propose a novel problem setting, Exploratory Grasping, for efficiently discovering reliable grasps on an unknown polyhedral object via sequential grasping, releasing, and toppling. We formalize Exploratory Grasping as a Markov Decision Process where we assume that the robot can (1) distinguish stable poses of a polyhedral object of unknown geometry, (2) generate grasp candidates on these poses and execute them, (3) determine whether each grasp is successful, and (4) release the object into a random new pose after a grasp successor topple the object after a grasp failure.  We study the theoretical complexity of Exploratory Grasping in the context of reinforcement learning and present an efficient bandit-style algorithm, Bandits for Online Rapid Grasp ExplorationStrategy (BORGES), which leverages the structure of the problem to efficiently discover high performing grasps for each object stable pose. BORGES can be used to complement any general-purpose grasping algorithm with any grasp modality (parallel-jaw, suction, multi-fingered, etc) to learn policies for objects in which they exhibit persistent failures. Simulation experiments suggest that BORGES can significantly outperform both general-purpose grasping pipelines and two other online learning algorithms and achieves performance within 5\% of the optimal policy within 1000 and 8000 timesteps on average across 46 challenging objects from the Dex-Net adversarial and EGAD! object datasets, respectively. Initial physical experiments suggesting that BORGES can improve grasp success rate on average over two challenging 3D printed objects by 45\% over a Dex-Net baseline. See https://tinyurl.com/exp-grasping for supplementary material and videos.},    
    paperid  =  {79}    
}    

@InProceedings{salter20,    
    title  =  {Attention-Privileged Reinforcement Learning},    
    author  =  {Salter, Sasha and Rao, Dushyant and Wulfmeier, Markus and Hadsell, Raia and Posner, Ingmar},    
    pages  =  {394--408},     
    abstract  =  {Image-based Reinforcement Learning is known to suffer from poor sample efficiency and generalisation to unseen visuals such as distractors (task-independent aspects of the observation space). Visual domain randomisation encourages transfer by training over visual factors of variation that may be encountered in the target domain. This increases learning complexity, can negatively impact learning rate and performance, and requires knowledge of potential variations during deployment. In this paper, we introduce Attention-Privileged Reinforcement Learning (APRiL) which uses a self-supervised attention mechanism to significantly alleviate these drawbacks: by focusing on task-relevant aspects of the observations, attention provides robustness to distractors as well as significantly increased learning efficiency. APRiL trains two attention-augmented actor-critic agents: one purely based on image observations, available across training and transfer domains; and one with access to privileged information (such as environment states) available only during training. Experience is shared between both agents and their attention mechanisms are aligned. The image-based policy can then be deployed without access to privileged information. We experimentally demonstrate accelerated and more robust learning on a diverse set of domains, leading to improved final performance for environments both within and outside the training distribution.},    
    paperid  =  {84}    
}    

@InProceedings{houston20,    
    title  =  {One Thousand and One Hours: Self-driving Motion Prediction Dataset},    
    author  =  {Houston, John and Zuidhof, Guido and Bergamini, Luca and Ye, Yawei and Chen, Long and Jain, Ashesh and Omari, Sammy and Iglovikov, Vladimir and Ondruska, Peter},    
    pages  =  {409--418},     
    abstract  =  {Motivated by the impact of large-scale datasets on ML systems we present the largest self-driving dataset for motion prediction to date, containing over 1,000 hours of data. This was collected by a fleet of 20 autonomous vehicles along a fixed route in Palo Alto, California, over a four-month period. It consists of 170,000 scenes, where each scene is 25 seconds long and captures the perception output of the self-driving system, which encodes the precise positions and motions of nearby vehicles, cyclists, and pedestrians over time. On top of this, the dataset contains a high-definition semantic map with 15,242 labelled elements and a high-definition aerial view over the area. We show that using a dataset of this size dramatically improves performance for key self-driving problems. Combined with the provided software kit, this collection forms the largest and most detailed dataset to date for the development of self-driving machine learning tasks, such as motion forecasting, motion planning and simulation.},    
    paperid  =  {86}    
}    

@InProceedings{yang20,    
    title  =  {Recovering and Simulating Pedestrians in the Wild},    
    author  =  {Yang, Ze and Manivasagam, Sivabalan and Liang, Ming and Yang, Bin and Ma, Wei-Chiu and Urtasun, Raquel},    
    pages  =  {419--431},     
    abstract  =  {Sensor simulation is a key component for testing the performance of self-driving vehicles and for data augmentation to better train perception systems. Typical approaches rely on artists to create both 3D assets and their animations to generate a new scenario. This, however, does not scale. In contrast, we propose to recover the shape and motion of pedestrians from sensor readings captured in the wild by a self-driving car driving around. Towards this goal, we formulate the problem as energy minimization in a deep structured model that exploits human shape priors, reprojection consistency with 2D poses extracted from images, and a ray-caster that encourages the reconstructed mesh to agree with the LiDAR readings. Importantly, we do not require any ground-truth 3D scans or 3D pose annotations. We then incorporate the reconstructed pedestrian assets bank in a realistic LiDAR simulation system by performing motion retargeting, and show that the simulated LiDAR data can be used to significantly reduce the amount of annotated real-world data required for visual perception tasks.},    
    paperid  =  {89}    
}    

@InProceedings{lin20,    
    title  =  {SoftGym: Benchmarking Deep Reinforcement Learning for Deformable Object Manipulation},    
    author  =  {Lin, Xingyu and Wang, Yufei and Olkin, Jake and Held, David},    
    pages  =  {432--448},     
    abstract  =  {Manipulating deformable objects has long been a challenge in robotics due to its high dimensional state representation and complex dynamics. Recent success in deep reinforcement learning provides a promising direction for learning to manipulate deformable objects with data driven methods. However, existing reinforcement learning benchmarks only cover tasks with direct state observability and simple low-dimensional dynamics or with relatively simple image-based environments, such as those with rigid objects. In this paper, we present SoftGym, a set of open-source simulated benchmarks for manipulating deformable objects, with a standard OpenAI Gym API and a Python interface for creating new environments. Our benchmark will enable reproducible research in this important area. Further, we evaluate a variety of algorithms on these tasks and highlight challenges for reinforcement learning algorithms, including dealing with a state representation that has a high intrinsic dimensionality and is partially observable. The experiments and analysis indicate the strengths and limitations of existing methods in the context of deformable object manipulation that can help point the way forward for future methods development.},    
    paperid  =  {91}    
}    

@InProceedings{vecerik20,    
    title  =  {S3K: Self-Supervised Semantic Keypoints for Robotic Manipulation via Multi-View Consistency},    
    author  =  {Vecerik, Mel and Regli, Jean-Baptiste and Sushkov, Oleg and Barker, David and Pevceviciute, Rugile and  Roth\"orl, Thomas and Hadsell, Raia and Agapito, Lourdes and Scholz, Jonathan},    
    pages  =  {449--460},     
    abstract  =  {A robot's ability to act is fundamentally constrained by what it can perceive. Many existing approaches to visual representation learning utilize general-purpose training criteria, e.g. image reconstruction, smoothness in latent space, or usefulness for control, or else make use of large datasets annotated with specific features (bounding boxes, segmentations, etc.). However, both approaches often struggle to capture the fine-detail required for precision tasks on specific objects, e.g. grasping and mating a plug and socket. We argue that these difficulties arise from a lack of geometric structure in these models. In this work we advocate semantic 3D keypoints as a visual representation, and present a semi-supervised training objective that can allow instance or category-level keypoints to be trained to 1-5 millimeter-accuracy with minimal supervision. Furthermore, unlike local texture-based approaches, our model integrates contextual information from a large area and is therefore robust to occlusion, noise, and lack of discernible texture. We demonstrate that this ability to locate semantic keypoints enables high level scripting of human understandable behaviours. Finally we show that these keypoints provide a good way to define reward functions for reinforcement learning and area good representation for training agents.},    
    paperid  =  {96}    
}    

@InProceedings{xiang20,    
    title  =  {Learning RGB-D Feature Embeddings for Unseen Object Instance Segmentation},    
    author  =  {Xiang, Yu and Xie, Christopher and Mousavian, Arsalan and Fox, Dieter},    
    pages  =  {461--470},     
    abstract  =  {Segmenting unseen objects in cluttered scenes is an important skill that robots need to acquire in order to perform tasks in new environments. In this work, we propose a new method for unseen object instance segmentation by learning RGB-D feature embeddings from synthetic data. A metric learning loss function is utilized to learn to produce pixel-wise feature embeddings such that pixels from the same object are close to each other and pixels from different objects are separated in the embedding space. With the learned feature embeddings, a mean shift clustering algorithm can be applied to discover and segment unseen objects. We further improve the segmentation accuracy with a new two-stage clustering algorithm. Our method demonstrates that non-photorealistic synthetic RGB and depth images can be used to learn feature embeddings that transfer well to real-world images for unseen object instance segmentation.},    
    paperid  =  {99}    
}    

@InProceedings{pirk20,    
    title  =  {Modeling Long-horizon Tasks as Sequential Interaction Landscapes},    
    author  =  {Pirk, Soeren and Hausman, Karol and Toshev, Alexander and Khansari, Mohi},    
    pages  =  {471--484},     
    abstract  =  {Task planning over long-time horizons is a challenging and open problem in robotics and its complexity grows exponentially with an increasing number of subtasks. In this paper we present a deep neural network that learns dependencies and transitions across subtasks solely from a set of demonstration videos. We represent each subtasks as action symbols (e.g. move cup), and show that these symbols can be learned and predicted directly from image observations. Learning symbol sequences provides the network with additional information about the most frequent transitions and relevant dependencies between subtasks and thereby structures tasks over long-time horizons. Learning from images, on the other hand, allows the network to continuously monitor the task progress and thus to interactively adapt to changes in the environment. We evaluate our framework on two long horizon tasks: (1) block stacking of puzzle pieces being executed by humans, and (2) a robot manipulation task involving pick and place of objects and sliding a cabinet door with  a 7-DoF robot arm. We show that complex plans can be carried out when executing the robotic task and the robot can interactively adapt to changes in the environment and recover from failure cases.},    
    paperid  =  {101}    
}    

@InProceedings{goyal20,    
    title  =  {PixL2R: Guiding Reinforcement Learning Using Natural Language by Mapping Pixels to Rewards},    
    author  =  {Goyal, Prasoon and Niekum, Scott and Mooney, Raymond},    
    pages  =  {485--497},     
    abstract  =  {Reinforcement learning (RL), particularly in sparse reward settings, often requires prohibitively large numbers of interactions with the environment, thereby limiting its applicability to complex problems. To address this, several prior approaches have used natural language to guide the agent's exploration. However, these approaches typically operate on structured representations of the environment, and/or assume some structure in the natural language commands. In this work, we propose a model that directly maps pixels to rewards, given a free-form natural language description of the task, which can then be used for policy learning. Our experiments on the Meta-World robot manipulation domain show that  language-based rewards significantly improves the sample efficiency of policy learning, both in sparse and dense reward settings.},    
    paperid  =  {104}    
}    

@InProceedings{ye20,    
    title  =  {Auxiliary Tasks Speed Up Learning Point Goal Navigation},    
    author  =  {Ye, Joel and Batra, Dhruv and Wijmans, Erik and Das, Abhishek},    
    pages  =  {498--516},     
    abstract  =  {PointGoal Navigation is an embodied task that requires agents to navigate to a specified point in an unseen environment. Wijmans et al. showed that this task is solvable in simulation but their method is computationally prohibitive - requiring 2.5 billion frames of experience and 180 GPU-days. We develop a method to significantly improve sample efficiency in learning PointNav using self-supervised auxiliary tasks (e.g. predicting the action taken between two egocentric observations, predicting the distance between two observations from a trajectory, etc.). We find that naively combining multiple auxiliary tasks improves sample efficiency, but only provides marginal gains beyond a point. To overcome this, we use attention to combine representations from individual auxiliary tasks. Our best agent is 5.5x faster to match the performance of the previous state-of-the-art, DD-PPO, at 40M frames, and improves on DD-PPO's performance at 40M frames by 0.16 SPL. Our code is publicly available at github.com/joel99/habitat-pointnav-aux.},    
    paperid  =  {107}    
}    

@InProceedings{pal20,    
    title  =  {Learning hierarchical relationships for object-goal navigation},    
    author  =  {Pal, Anwesan and Qiu, Yiding and Christensen, Henrik},    
    pages  =  {517--528},     
    abstract  =  {Direct search for objects as part of navigation poses a challenge for small items. Utilizing context in the form of object-object relationships enable hierarchical search for targets efficiently. Most of the current approaches tend to directly incorporate sensory input into a reward-based learning approach, without learning about object relationships in the natural environment, and thus generalize poorly across domains. We present Memory-utilized Joint hierarchical Object Learning for Navigation in Indoor Rooms (MJOLNIR), a target-driven navigation algorithm, which considers the inherent relationship between target objects, and the more salient contextual objects occurring in its surrounding. Extensive experiments conducted across multiple environment settings show an 82.9\% and 93.5\% gain over existing state-of-the-art navigation methods in terms of the success rate (SR), and success weighted by path length (SPL), respectively. We also show that our model learns to converge much faster than other algorithms, without suffering from the well-known overfitting problem. Additional details regarding the supplementary material and code are available at https://sites.google.com/eng.ucsd.edu/mjolnir.},    
    paperid  =  {108}    
}    

@InProceedings{ni20,    
    title  =  {f-IRL: Inverse Reinforcement Learning via State Marginal Matching},    
    author  =  {Ni, Tianwei and Sikchi, Harshit and Wang, Yufei and Gupta, Tejus and Lee, Lisa and Eysenbach, Ben},    
    pages  =  {529--551},     
    abstract  =  {Imitation learning is well-suited for robotic tasks where it is difficult to directly program the behavior or specify a cost for optimal control.  In this work, we propose a method for learning the reward function (and the corresponding policy) to match the expert state density. Our main result is the analytic gradient of any f-divergence between the agent and expert state distribution w.r.t. reward parameters. Based on the derived gradient, we present an algorithm, f-IRL, that recovers a stationary reward function from the expert density by gradient descent. We show that f-IRL can learn behaviors from a hand-designed target state density or implicitly through expert observations. Our method outperforms adversarial imitation learning methods in terms of sample efficiency and the required number of expert trajectories on IRL benchmarks. Moreover, we show that the recovered reward function can be used to quickly solve downstream tasks, and empirically demonstrate its utility on hard-to-explore tasks and for behavior transfer across changes in dynamics. Project videos and code link are available at https://sites.google.com/view/f-irl/home.},    
    paperid  =  {111}    
}    

@InProceedings{georgiev20,    
    title  =  {Iterative Semi-parametric Dynamics Model Learning For Autonomous Racing},    
    author  =  {Georgiev, Ignat and Chatzikomis, Christoforos and Voelkl, Timo and Smith, Joshua and Mistry, Michael},    
    pages  =  {552--563},     
    abstract  =  {Accurately modeling robot dynamics is crucial to safe and efficient motion control. In this paper, we develop and apply an iterative learning semi-parametric model, with a neural network, to the task of autonomous racing with a Model Predictive  Controller (MPC). We present a novel non-linear semi-parametric dynamics model where we represent the known dynamics with a parametric model, and a neural network captures the unknown dynamics. We show that our model can learn more accurately than a purely parametric model and generalize better than a purely non-parametric model, making it ideal for real-world applications where collecting data from the full state space is not feasible. We present a system where the model is bootstrapped on pre-recorded data and then updated iteratively at run time. Then we apply our iterative learning approach to the simulated problem of autonomous racing and show that it can safely adapt to modified dynamics online and even achieve better performance than models trained on data from manual driving.},    
    paperid  =  {112}    
}    

@InProceedings{yan20,    
    title  =  {Learning Predictive Representations for Deformable Objects Using Contrastive Estimation},    
    author  =  {Yan, Wilson and Vangipuram, Ashwin and Abbeel, Pieter and Pinto, Lerrel},    
    pages  =  {564--574},     
    abstract  =  {Using visual model-based learning for deformable object manipulation is challenging due to difficulties in learning plannable visual representations along with complex dynamic models. In this work, we propose a new learning framework that jointly optimizes both the visual representation model and the dynamics model using contrastive estimation. Using simulation data collected by randomly perturbing deformable objects on a table, we learn latent dynamics models for these objects in an offline fashion. Then, using the learned models, we use simple model-based planning to solve challenging deformable object manipulation tasks such as spreading ropes and cloths. Experimentally, we show substantial improvements in performance over standard model-based learning techniques across our rope and cloth manipulation suite. Finally, we transfer our visual manipulation policies trained on data purely collected in simulation to a real PR2 robot through domain randomization.},    
    paperid  =  {126}    
}    

@InProceedings{xie20a,    
    title  =  {Learning Latent Representations to Influence Multi-Agent Interaction},    
    author  =  {Xie, Annie and Losey, Dylan and Tolsma, Ryan and Finn, Chelsea and Sadigh, Dorsa},    
    pages  =  {575--588},     
    abstract  =  {Seamlessly interacting with humans or robots is hard because these agents are non-stationary. They update their policy in response to the ego agent's behavior, and the ego agent must anticipate these changes to co-adapt. Inspired by humans, we recognize that robots do not need to explicitly model every low-level action another agent will make; instead, we can capture the latent strategy of other agents through high-level representations. We propose a reinforcement learning-based framework for learning latent representations of an agent's policy, where the ego agent identifies the relationship between its behavior and the other agent's future strategy. The ego agent then leverages these latent dynamics to influence the other agent, purposely guiding them towards policies suitable for co-adaptation. Across several simulated domains and a real-world air hockey game, our approach outperforms the alternatives and learns to influence the other agent.},    
    paperid  =  {128}    
}    

@InProceedings{yamada20,    
    title  =  {Motion Planner Augmented Reinforcement Learning for Robot Manipulation in Obstructed Environments},    
    author  =  {Yamada, Jun and Lee, Youngwoon and Salhotra, Gautam and Pertsch, Karl and Pflueger, Max and Sukhatme, Gaurav and Lim, Joseph and Englert, Peter},    
    pages  =  {589--603},     
    abstract  =  {Deep reinforcement learning (RL) agents are able to learn contact-rich manipulation tasks by maximizing a reward signal, but require large amounts of experience, especially in environments with many obstacles that complicate exploration. In contrast, motion planners use explicit models of the agent and environment to plan collision-free paths to faraway goals, but suffer from inaccurate models in tasks that require contacts with the environment. To combine the benefits of both approaches, we propose motion planner augmented RL (MoPA-RL) which augments the action space of an RL agent with the long-horizon planning capabilities of motion planners. Based on the magnitude of the action, our approach smoothly transitions between directly executing the action and invoking a motion planner. We evaluate our approach on various simulated manipulation tasks and compare it to alternative action spaces in terms of learning efficiency and safety. The experiments demonstrate that MoPA-RL increases learning efficiency, leads to a faster exploration, and results in safer policies that avoid collisions with the environment. Videos and code are available at https://clvrai.com/mopa-rl.},    
    paperid  =  {130}    
}    

@InProceedings{cui20,    
    title  =  {The EMPATHIC Framework for Task Learning from Implicit Human Feedback},    
    author  =  {Cui, Yuchen and Zhang, Qiping and Knox, Brad and Allievi, Alessandro and Stone, Peter and Niekum, Scott},    
    pages  =  {604--626},     
    abstract  =  {Reactions such as gestures, facial expressions, and vocalizations are an abundant, naturally occurring channel of information that humans provide during interactions. A robot or other agent could leverage an understanding of such implicit human feedback to improve its task performance at no cost to the human. This approach contrasts with common agent teaching methods based on demonstrations, critiques, or other guidance that need to be attentively and intentionally provided. In this paper, we first define the general problem of learning from implicit human feedback and then propose to address this problem through a novel data-driven framework, EMPATHIC. This two-stage method consists of (1) mapping implicit human feedback to relevant task statistics such as reward, optimality, and advantage; and (2) using such a mapping to learn a task. We instantiate the first stage and three second-stage evaluations of the learned mapping. To do so, we collect a dataset of human facial reactions while subjects observe an agent execute a sub-optimal policy for a prescribed training task. We train a deep neural network on this data and demonstrate its ability to (1) infer relative reward ranking of events in the training task from prerecorded human facial reactions; (2) improve the policy of an agent in the training task using live human facial reactions; and (3) transfer to a novel domain in which it evaluates robot manipulation trajectories.},    
    paperid  =  {132}    
}    

@InProceedings{bewley20,    
    title  =  {Range Conditioned Dilated Convolutions for Scale Invariant 3D Object Detection},    
    author  =  {Bewley, Alex and Sun, Pei and Mensink, Thomas and Anguelov, Dragomir and Sminchisescu, Cristian},    
    pages  =  {627--641},     
    abstract  =  {This paper presents a novel 3D object detection framework that processes LiDAR data directly on its native representation: range images. Benefiting from the compactness of range images, 2D convolutions can efficiently process dense LiDAR data of the scene. To overcome scale sensitivity in this perspective view, a novel range-conditioned dilation (RCD) layer is proposed to dynamically adjust a continuous dilation rate as a function of the measured range. Furthermore, localized soft range gating combined with a 3D box-refinement stage improves robustness in occluded areas, and produces overall more accurate bounding box predictions. On the public large-scale Waymo Open Dataset, our method sets a new baseline for range-based 3D detection, outperforming multiview and voxel-based methods over all ranges with unparalleled performance at long range detection.},    
    paperid  =  {134}    
}    

@InProceedings{ploeger20,    
    title  =  {High Acceleration Reinforcement Learning for Real-World Juggling with Binary Rewards},    
    author  =  {Ploeger, Kai and Lutter, Michael and Peters, Jan},    
    pages  =  {642--653},     
    abstract  =  {Robots that can learn in the physical world will be important to enable robots to escape their stiff and pre-programmed movements.  For dynamic high-acceleration tasks, such as juggling, learning in the real-world is particularly challenging  as  one  must  push  the  limits  of  the  robot  and  its  actuation  without harming the system, amplifying the necessity of sample efficiency and safety for robot learning algorithms.  In contrast to prior work which mainly focuses on the learning algorithm, we propose a learning system, that directly incorporates these requirements in the design of the policy representation,  initialization,  and optimization.  We demonstrate that this system enables the high-speed Barrett WAM manipulator to learn juggling two balls from 56 minutes of experience with a binary reward signal and finally juggles continuously for up to 33 minutes or about 4500 repeated catches. The videos documenting the learning process and the evaluation can be found at https://sites.google.com/view/jugglingbot},    
    paperid  =  {135}    
}    

@InProceedings{dean20,    
    title  =  {Guaranteeing Safety of Learned Perception Modules via Measurement-Robust Control Barrier Functions},    
    author  =  {Dean, Sarah and Taylor, Andrew and Cosner, Ryan and Recht, Benjamin and Ames, Aaron},    
    pages  =  {654--670},     
    abstract  =  {Modern nonlinear control theory seeks to develop feedback controllers that endow systems with properties such as safety and stability. The guarantees ensured by these controllers often rely on accurate estimates of the system state for determining control actions. In practice, measurement model uncertainty can lead to error in state estimates that degrades these guarantees. In this paper, we seek to unify techniques from control theory and machine learning to synthesize controllers that achieve safety in the presence of measurement model uncertainty. We define the notion of a Measurement-Robust Control Barrier Function (MR-CBF) as a tool for determining safe control inputs when facing measurement model uncertainty. Furthermore, MR-CBFs are used to inform sampling methodologies for learning-based perception systems and quantify tolerable error in the resulting learned models. We demonstrate the efficacy of MR-CBFs in achieving safety with measurement model uncertainty on a simulated Segway system.},    
    paperid  =  {140}    
}    

@InProceedings{anderson20,    
    title  =  {Sim-to-Real Transfer for Vision-and-Language Navigation},    
    author  =  {Anderson, Peter and Shrivastava, Ayush and Truong, Joanne and Majumdar, Arjun and Parikh, Devi and Batra, Dhruv and Lee, Stefan},    
    pages  =  {671--681},     
    abstract  =  {We study the challenging problem of releasing a robot in a previously unseen environment, and having it follow unconstrained natural language navigation instructions. Recent work on the task of Vision-and-Language Navigation (VLN) has achieved significant progress in simulation. To assess the implications of this work for robotics, we transfer a VLN agent trained in simulation to a physical robot. To bridge the gap between the high-level discrete action space learned by the VLN agent, and the robot's low-level continuous action space, we propose a subgoal model to identify nearby waypoints, and use domain randomization to mitigate visual domain differences. For accurate sim and real comparisons in parallel environments, we annotate a 325m2 office space with 1.3km of navigation instructions, and create a digitized replica in simulation. We find that sim-to-real transfer to an environment not seen in training is successful if an occupancy map and navigation graph can be collected and annotated in advance (success rate of 46.8\% vs. 55.9\% in sim), but much more challenging in the hardest setting with no prior mapping at all (success rate of 22.5\%).},    
    paperid  =  {142}    
}    

@InProceedings{jauhri20,    
    title  =  {Interactive Imitation Learning in State-Space},    
    author  =  {Jauhri, Snehal and Celemin, Carlos and Kober, Jens},    
    pages  =  {682--692},     
    abstract  =  {Imitation Learning techniques enable programming the behaviour of agents through demonstrations rather than manual engineering. However, they are limited by the quality of available demonstration data. Interactive Imitation Learning techniques can improve the efficacy of learning since they involve teachers providing feedback while the agent executes its task. In this work, we propose a novel Interactive Learning technique that uses human feedback in state-space to train and improve agent behaviour (as opposed to alternative methods that use feedback in action-space). Our method titled Teaching Imitative Policies in State-space (TIPS) enables providing guidance to the agent in terms of `changing its state' which is often more intuitive for a human demonstrator. Through continuous improvement via corrective feedback, agents trained by non-expert demonstrators using TIPS outperformed the demonstrator and conventional Imitation Learning agents.},    
    paperid  =  {147}    
}    

@InProceedings{manuelli20,    
    title  =  {Keypoints into the Future: Self-Supervised Correspondence in Model-Based Reinforcement Learning},    
    author  =  {Manuelli, Lucas and Li, Yunzhu and Florence, Pete and Tedrake, Russ},    
    pages  =  {693--710},     
    abstract  =  {Predictive models have been at the core of many robotic systems, from quadrotors to walking robots. However, it has been challenging to develop and apply such models to practical robotic manipulation due to high-dimensional sensory observations such as images. Previous approaches to learning models in the context of robotic manipulation have either learned whole image dynamics or used autoencoders to learn dynamics in a low-dimensional latent state. In this work, we introduce model-based prediction with self-supervised visual correspondence learning, and show that not only is this indeed possible, but demonstrate that these types of predictive models show compelling performance improvements over alternative methods for vision-based RL with autoencoder-type vision training. Through simulation experiments, we demonstrate that our models provide better generalization precision, particularly in 3D scenes, scenes involving occlusion, and in category-generalization. Additionally, we validate that our method effectively transfers to the real world through hardware experiments. https://sites.google.com/view/keypointsintothefuture},    
    paperid  =  {149}    
}    

@InProceedings{wang20d,    
    title  =  {Model-based Reinforcement Learning for Decentralized Multiagent Rendezvous},    
    author  =  {Wang, Rose and Kew, J. Chase and Lee, Dennis and Lee, Tsang-Wei and Zhang, Tingnan and Ichter, Brian and Tan, Jie and Faust, Aleksandra},    
    pages  =  {711--725},     
    abstract  =  {Collaboration requires agents to align their goals on the fly. Underlying the human ability to align goals with other agents is their ability to predict the intentions of others and actively update their own plans. We propose hierarchical predictive planning (HPP), a model-based reinforcement learning method for decentralized multiagent rendezvous. Starting with pretrained, single-agent point to point navigation policies  and using noisy, high-dimensional sensor inputs like lidar, we first learn via self-supervision motion predictions of all agents on the team. Next, HPP uses the prediction models to propose and evaluate navigation subgoals for completing the rendezvous task without explicit communication among agents. We evaluate HPP in a suite of unseen environments, with increasing complexity and numbers of obstacles. We show that HPP outperforms alternative reinforcement learning, path planning, and heuristic-based baselines on challenging, unseen environments. Experiments in the real world demonstrate successful transfer of the prediction models from sim to real world without any additional fine-tuning. Altogether, HPP removes the need for a centralized operator in multiagent systems by combining model-based RL and inference methods, enabling agents to dynamically align plans.},    
    paperid  =  {151}    
}    

@InProceedings{zeng20,    
    title  =  {Transporter Networks: Rearranging the Visual World for Robotic Manipulation},    
    author  =  {Zeng, Andy and Florence, Pete and Tompson, Jonathan and Welker, Stefan and Chien, Jonathan and Attarian, Maria and Armstrong, Travis and Krasin, Ivan and Duong, Dan and Sindhwani, Vikas and Lee, Johnny},    
    pages  =  {726--747},     
    abstract  =  {Robotic manipulation can be formulated as inducing a sequence of spatial displacements: where the space being moved can encompass an object, part of an object, or end effector. In this work, we propose the Transporter Network, a simple model architecture that rearranges deep features to infer spatial displacements from visual input -- which can parameterize robot actions. It makes no assumptions of objectness (e.g. canonical poses, models, or keypoints), it exploits spatial symmetries, and is orders of magnitude more sample efficient than our benchmarked alternatives in learning vision-based manipulation tasks: from stacking a pyramid of blocks, to assembling kits with unseen objects; from manipulating deformable ropes, to pushing piles of small objects with closed-loop feedback. Our method can represent complex multi-modal policy distributions and generalizes to multi-step sequential tasks, as well as 6DoF pick-and-place. Experiments on 10 simulated tasks show that it learns faster and generalizes better than a variety of end-to-end baselines, including policies that use ground-truth object poses. We validate our methods with hardware in the real world. Experiment videos and code will be made available at https://transporternets.github.io},    
    paperid  =  {153}    
}    

@InProceedings{reddy20,    
    title  =  {Assisted Perception: Optimizing Observations to Communicate State},    
    author  =  {Reddy, Siddharth and Levine, Sergey and Dragan, Anca},    
    pages  =  {748--764},     
    abstract  =  {We aim to help users estimate the state of the world in tasks like robotic teleoperation and navigation with visual impairment, where user may have systematic biases that lead to suboptimal behavior: they might struggle to process observations from multiple sensors simultaneously, receive delayed observations, or underestimate distances to obstacles. While we cannot directly change the user's internal beliefs or their internal state estimation process, our insight is that we can still assist them by modifying the user's observations. Instead of showing the user their true observations, ***we synthesize new observations that lead to more accurate internal state estimates when processed by the user***. We refer to this method as assistive state estimation (ASE): an automated assistant uses the true observations to infer the state of the world, then generates a modified observation for the user to consume (e.g., through an augmented reality interface), and optimizes the modification to induce the user's new beliefs to match the assistant's current beliefs. To predict the effect of the modified observation on the user's beliefs, ASE learns a model of the user's state estimation process: after each task completion, it searches for a model that would have led to beliefs that explain the user's actions. We evaluate ASE in a user study with 12 participants who each perform four tasks: two tasks with known user biases -- bandwidth-limited image classification and a driving video game with observation delay -- and two with unknown biases that our method has to learn -- guided 2D navigation and a lunar lander teleoperation video game. ASE's general-purpose approach to synthesizing informative observations enables a different assistance strategy to emerge in each domain, such as quickly revealing informative pixels to speed up image classification, using a dynamics model to undo observation delay in driving, identifying nearby landmarks for navigation, and exaggerating a visual indicator of tilt in the lander game. The results show that ASE substantially improves the task performance of users with bandwidth constraints, observation delay, and other unknown biases.},    
    paperid  =  {158}    
}    

@InProceedings{shaj20,    
    title  =  {Action-Conditional Recurrent Kalman Networks For Forward and Inverse Dynamics Learning},    
    author  =  {Shaj, Vaisakh and Becker, Philipp and B\"{u}chler, Dieter and Pandya, Harit and Duijkeren, Niels van and Taylor, C. James and Hanheide, Marc and Neumann, Gerhard},    
    pages  =  {765--781},     
    abstract  =  {Estimating accurate forward and inverse dynamics models is a crucial component of model-based control for sophisticated robots such as robots driven by hydraulics, artificial muscles, or robots dealing with different contact situations. Analytic models to such processes are often unavailable or inaccurate due to complex hysteresis effects, unmodelled friction and stiction phenomena, and unknown effects during contact situations. A promising approach is to obtain spatio-temporal models in a data-driven way using recurrent neural networks, as they can overcome those issues. However, such models often do not meet accuracy demands sufficiently, degenerate in performance for the required high sampling frequencies and cannot provide uncertainty estimates.  We adopt a recent probabilistic recurrent neural network architecture, called Recurrent Kalman Networks (RKNs), to model learning by conditioning its transition dynamics on the control actions. RKNs outperform standard recurrent networks such as LSTMs on many state estimation tasks.Inspired by Kalman filters, the RKN provides an elegant way to achieve action conditioning within its recurrent cell by leveraging additive interactions between the current latent state and the action variables. We present two architectures, one for forward model learning and one for inverse model learning. Both architectures significantly outperform existing model learning frameworks as well as analytical models in terms of prediction performance on a variety of real robot dynamics models.},    
    paperid  =  {159}    
}    

@InProceedings{grannen20,    
    title  =  {Untangling Dense Knots by Learning Task-Relevant Keypoints},    
    author  =  {Grannen, Jennifer and Sundaresan, Priya and Thananjeyan, Brijen and Ichnowski, Jeffrey and Balakrishna, Ashwin and Viswanath, Vainavi and Laskey, Michael and Gonzalez, Joseph and Goldberg, Ken},    
    pages  =  {782--800},     
    abstract  =  {Untangling ropes, wires, and cables is a challenging task for robots due to the high-dimensional configuration space, visual homogeneity, self-occlusions, and complex dynamics. We consider dense (tight) knots that lack space between self-intersections and present an iterative approach that uses learned geometric structure in configurations. We instantiate this into an algorithm, HULK: Hierarchical Untangling from Learned Keypoints, which combines learning-based perception with a geometric planner into a policy that guides a bilateral robot to untangle knots. To evaluate the policy, we perform experiments both in a novel simulation environment modelling cables with varied knot types and textures and in a physical system using the da Vinci surgical robot. We find that HULK is able to untangle cables with dense figure-eight and overhand knots and generalize to varied textures and appearances. We compare two variants of HULK to three baselines and observe that HULK achieves 43.3\% higher success rates on a physical system compared to the next best baseline. HULK successfully untangles a cable from a dense initial configuration containing up to two overhand and figure-eight knots in 97.9\% of 378 simulation experiments with an average of 12.1 actions per trial. In physical experiments, HULK achieves 61.7\% untangling success, averaging 8.48 actions per trial. Supplementary material, code, and videos can be found at https://tinyurl.com/y3a88ycu.},    
    paperid  =  {166}    
}    

@InProceedings{chow20,    
    title  =  {Safe Policy Learning for Continuous Control},    
    author  =  {Chow, Yinlam and Nachum, Ofir and Faust, Aleksandra and Due\~{n}ez-Guzman, Edgar and Ghavamzadeh, Mohammad},    
    pages  =  {801--821},     
    abstract  =  {We study continuous action reinforcement learning problems in which it is crucial that the agent interacts with the environment only through near-safe policies, i.e.,~policies that keep the agent in desirable situations, both during training and at convergence. We formulate these problems as {\em constrained} Markov decision processes (CMDPs) and present safe policy optimization algorithms that are based on a Lyapunov approach to solve them. Our algorithms can use any standard policy gradient (PG) method, such as deep deterministic policy gradient (DDPG) or proximal policy optimization (PPO), to train a neural network policy, while enforcing near-constraint satisfaction for every policy update by projecting either the policy parameter or the selected action onto the set of feasible solutions induced by the state-dependent linearized Lyapunov constraints. Compared to the existing constrained PG algorithms, ours are more data efficient as they are able to utilize both on-policy and off-policy data. Moreover, in practice our action-projection algorithm often leads to less conservative policy updates and allows for natural integration into an end-to-end PG training pipeline. We evaluate our algorithms and compare them with the state-of-the-art baselines on several simulated (MuJoCo) tasks, as well as a real-world robot obstacle-avoidance problem, demonstrating their effectiveness in terms of balancing performance and constraint satisfaction.},    
    paperid  =  {171}    
}    

@InProceedings{sharma20a,    
    title  =  {Learning to Compose Hierarchical Object-Centric Controllers for Robotic Manipulation},    
    author  =  {Sharma, Mohit and Liang, Jacky and Zhao, Jialiang and Lagrassa, Alex and Kroemer, Oliver},    
    pages  =  {822--844},     
    abstract  =  {Manipulation tasks can often be decomposed into multiple subtasks performed in parallel, e.g., sliding an object to a goal pose while maintaining contact with a table.  Individual subtasks can be achieved by task-axis controllers defined relative to the objects being manipulated, and a set of object-centric controllers can be combined in an hierarchy. In prior works, such combinations are defined manually or learned from demonstrations. By contrast, we propose using reinforcement learning to dynamically compose hierarchical object-centric controllers for manipulation tasks. Experiments in both simulation and real world show how the proposed approach leads to improved sample efficiency, zero-shot generalization to novel test environments, and simulation-to-reality transfer without fine-tuning.},    
    paperid  =  {176}    
}    

@InProceedings{sharma20b,    
    title  =  {Relational Learning for Skill Preconditions},    
    author  =  {Sharma, Mohit and Kroemer, Oliver},    
    pages  =  {845--861},     
    abstract  =  {To determine if a skill can be executed in any given environment, a robot needs to learn the preconditions for the skill. As robots begin to operate in dynamic and unstructured environments, these precondition models will need to generalize to variable number of objects with different shapes and sizes. In this work, we focus on learning precondition models for manipulation skills in unconstrained environments. Our work is motivated by the intuition that many complex manipulation tasks, with multiple objects, can be simplified by focusing on less complex pairwise object relations. We propose an object-relation model that learns continuous representations for these pairwise object relations.  Our object-relation model is trained completely in simulation, and once learned, is used by a separate precondition model to predict skill preconditions for real world tasks. We evaluate our precondition model on 3 different manipulation tasks: sweeping, cutting, and unstacking.  We show that our approach leads to significant improvements in predicting preconditions for all 3 tasks, across objects of different shapes and sizes.},    
    paperid  =  {178}    
}    

@InProceedings{brito20,    
    title  =  {Social-VRNN: One-Shot Multi-modal Trajectory Prediction for Interacting Pedestrians},    
    author  =  {Brito, Bruno Ferreira de and Zhu, Hai and Pan, Wei and Alonso-Mora, Javier},    
    pages  =  {862--872},     
    abstract  =  {Prediction of human motions is key for safe navigation of autonomous robots among humans. In cluttered environments, several motion hypotheses may exist for a pedestrian, due to its interactions with the environment and other pedestrians. Previous works for estimating multiple motion hypotheses require a large number of samples which limits their applicability in real-time motion planning. In this paper, we present a variational learning approach for interaction-aware and multi-modal trajectory prediction based on deep generative neural networks. Our approach can achieve faster convergence and requires significantly fewer samples comparing to state-of-the-art methods. Experimental results on real and simulation data show that our model can effectively learn to infer different trajectories. We compare our method with three baseline approaches and present performance results demonstrating that our generative model can achieve higher accuracy for trajectory prediction by producing diverse trajectories.},    
    paperid  =  {182}    
}    

@InProceedings{xie20b,    
    title  =  {MuGNet: Multi-Resolution Graph Neural Network for Segmenting Large-Scale Pointclouds},    
    author  =  {Xie, Liuyue and Furuhata, Tomotake and Shimada, Kenji},    
    pages  =  {873--882},     
    abstract  =  {In this paper, we propose a multi-resolution deep-learning architecture to segment dense large-scale pointclouds semantically. Dense pointcloud data require a computationally expensive feature encoding process before semantic segmentation. Previous work has used different approaches to drastically downsample from the original pointcloud to utilize common computing hardware. While these approaches can relieve the computation burden to some extent, they are still limited in their processing capability for multiple scans. We present MuGNet, a memory-efficient, end-to-end graph neural network framework to perform semantic segmentation on large-scale pointclouds. We reduce the computation demand by utilizing a graph neural network on the preformed pointcloud graphs and retain the segmentation's precision with a bidirectional network that fuses feature embedding at different resolutions. Our framework has been validated on benchmark datasets, including Stanford Large-Scale 3D Indoor Spaces Dataset(S3DIS) and Virtual KITTI Dataset. We demonstrate that our framework can process up to 45 room scans at once on a single 11 GB GPU while still surpassing other graph-based solutions for segmentation on S3DIS with an 88.5\% (+3\%) overall accuracy and 69.8\% (+7.7\%) mIOU accuracy.},    
    paperid  =  {184}    
}    

@InProceedings{da20,    
    title  =  {Learning a Contact-Adaptive Controller for Robust, Efficient Legged Locomotion},    
    author  =  {Da, Xingye and Xie, Zhaoming and Hoeller, David and Boots, Byron and Anandkumar, Anima and Zhu, Yuke and Babich, Buck and Garg, Animesh},    
    pages  =  {883--894},     
    abstract  =  {We present a hierarchical framework that combines model-based control and reinforcement learning (RL) to synthesize robust controllers for a quadruped (the Unitree Laikago). The system consists of a high-level controller that learns to choose from a set of primitives in response to changes in the environment and a low-level controller that utilizes an established control method to robustly execute the primitives. Our framework learns a controller that can adapt to challenging environmental changes on the fly, including novel scenarios not seen during training. The learned controller is up to 85~percent more energy efficient and is more robust compared to baseline methods. We also deploy the controller on a physical robot without any randomization or adaptation scheme.},    
    paperid  =  {187}    
}    

@InProceedings{zhao20b,    
    title  =  {TNT: Target-driven Trajectory Prediction},    
    author  =  {Zhao, Hang and Gao, Jiyang and Lan, Tian and Sun, Chen and Sapp, Ben and Varadarajan, Balakrishnan and Shen, Yue and Shen, Yi and Chai, Yuning and Schmid, Cordelia and Li, Congcong and Anguelov, Dragomir},    
    pages  =  {895--904},     
    abstract  =  {Predicting the future behavior of moving agents is essential for real world applications. It is challenging as the intent of the agent and the corresponding behavior is unknown and intrinsically multimodal. Our key insight is that for prediction within a moderate time horizon, the future modes can be effectively captured by a set of target states. This leads to our target-driven trajectory prediction (TNT) framework. TNT has three stages which are trained end-to-end. It first predicts an agent's potential target states T steps into the future, by encoding its interactions with the environment and the other agents. TNT then generates trajectory state sequences conditioned on targets. A final stage estimates trajectory likelihoods and a final compact set of trajectory predictions is selected. This is in contrast to previous work which models agent intents as latent variables, and relies on test-time sampling to generate diverse trajectories. We benchmark TNT on trajectory prediction of vehicles and pedestrians, where we achieve better than state-of-the-art performance on Argoverse Forecasting, INTERACTION, Stanford Drone and an in-house Pedestrian-at-Intersection dataset.},    
    paperid  =  {189}    
}    

@InProceedings{han20,    
    title  =  {Planning Paths Through Unknown Space by Imagining What Lies Therein},    
    author  =  {Han, Yutao and Banfi, Jacopo and Campbell, Mark},    
    pages  =  {905--914},     
    abstract  =  {This paper presents a novel framework for planning paths in maps containing unknown spaces, such as from occlusions. Our approach takes as input a semantically-annotated point cloud, and leverages an image inpainting neural network to generate a reasonable model of unknown space as free or occupied. Our validation campaign shows that it is possible to greatly increase the performance of standard pathfinding algorithms which adopt the general optimistic assumption of treating unknown space as free.},    
    paperid  =  {195}    
}    

@InProceedings{jeong20,    
    title  =  {Learning Dexterous Manipulation from Suboptimal Experts},    
    author  =  {Jeong, Rae and Springenberg, Jost Tobias and Kay, Jackie and Zheng, Dan and Galashov, Alexandre and Heess, Nicolas and Nori, Francesco},    
    pages  =  {915--934},     
    abstract  =  {Learning dexterous manipulation in high-dimensional state-action spaces is an important open challenge with exploration presenting a major bottleneck. Although in many cases the learning process could be guided by demonstrations or other suboptimal experts, current RL algorithms for continuous action spaces often fail to effectively utilize combinations of highly off-policy expert data and on-policy exploration data. As a solution, we introduce Relative Entropy Q-Learning (REQ), a simple policy iteration algorithm that combines ideas from successful offline and conventional RL algorithms. It represents the optimal policy via importance sampling from a learned prior and is well-suited to take advantage of mixed data distributions. We demonstrate experimentally that REQ outperforms several strong baselines on robotic manipulation tasks for which suboptimal experts are available. We show how suboptimal experts can be constructed effectively by composing simple waypoint tracking controllers, and we also show how learned primitives can be combined with waypoint controllers to obtain reference behaviors to bootstrap a complex manipulation task on a simulated bimanual robot with human-like hands. Finally, we show that REQ is also effective for general off-policy RL, offline RL, and RL from demonstrations.},    
    paperid  =  {198}    
}    

@InProceedings{matl20,    
    title  =  {STReSSD: Sim-To-Real from Sound for Stochastic Dynamics},    
    author  =  {Matl, Carolyn and Narang, Yashraj and Fox, Dieter and Bajcsy, Ruzena and Ramos, Fabio},    
    pages  =  {935--958},     
    abstract  =  {Sound is an information-rich medium that captures dynamic physical events. This work presents STReSSD, a framework that uses sound to bridge the simulation-to-reality gap for stochastic dynamics, demonstrated for the canonical case of a bouncing ball. A physically-motivated noise model is presented to capture stochastic behavior of the balls upon collision with the environment. A likelihood-free Bayesian inference framework is used to infer the parameters of the noise model, as well as a material property called the coefficient of restitution, from audio observations.  The same inference framework and the calibrated stochastic simulator are then used to learn a probabilistic model of ball dynamics.  The predictive capabilities of the dynamics model are tested in two robotic experiments. First, open-loop predictions anticipate probabilistic success of bouncing a ball into a cup. The second experiment integrates audio perception with a robotic arm to track and deflect a bouncing ball in real-time. We envision that this work is a step towards integrating audio-based inference for dynamic robotic tasks.},    
    paperid  =  {202}    
}    

@InProceedings{ma20,    
    title  =  {Contrastive Variational Reinforcement Learning for Complex Observations},    
    author  =  {Ma, Xiao and CHEN, SIWEI and Hsu, David and Lee, Wee Sun},    
    pages  =  {959--972},     
    abstract  =  {Deep reinforcement learning (DRL) has achieved significant success in various robot tasks: manipulation, navigation, etc. However, complex visual observations in natural environments remains a major challenge. This paper presents Contrastive Variational Reinforcement Learning (CVRL), a model-based method that tackles complex visual observations in  DRL.  CVRL learns a contrastive variational model by maximizing the mutual information between latent states and observations discriminatively, through contrastive learning. It avoids modeling the complex observation space unnecessarily, as the commonly used generative observation model often does,  and is significantly more robust. CVRL achieves comparable performance with state-of-the-art model-based DRL methods on standard Mujoco tasks. It significantly outperforms them on Natural Mujoco tasks and a robot box-pushing task with complex observations, e.g., dynamic shadows. The CVRL code is available publicly at https://github.com/Yusufma03/CVRL.},    
    paperid  =  {203}    
}    

@InProceedings{segal20,    
    title  =  {Universal Embeddings for Spatio-Temporal Tagging of Self-Driving Logs},    
    author  =  {Segal, Sean and Kee, Eric and Luo, Wenjie and Sadat, Abbas and Yumer, Ersin and Urtasun, Raquel},    
    pages  =  {973--983},     
    abstract  =  {In this paper, we tackle the problem of spatio-temporal tagging of self-driving scenes from raw sensor data. Our approach learns a universal embedding for all tags, enabling efficient tagging of many attributes and faster learning of new attributes with limited data. Importantly, the embedding is spatio-temporally aware, allowing the model to naturally output spatio-temporal tag values. Values can then be pooled over arbitrary regions, in order to, for example, compute the pedestrian density in front of the SDV, or determine if a car is blocking another car at a 4-way intersection. We demonstrate the effectiveness of our approach on a new large scale self-driving dataset, SDVScenes, containing 15 attributes relating to vehicle and pedestrian density, the actions of each actor, the speed of each actor, interactions between actors, and the topology of the road map.},    
    paperid  =  {205}    
}    

@InProceedings{joshi20,    
    title  =  {Asynchronous Deep Model Reference Adaptive Control},    
    author  =  {Joshi, Girish and Virdi, Jasvir and Chowdhary, Girish},    
    pages  =  {984--1000},     
    abstract  =  {In this paper, we present Asynchronous implementation of Deep Neural Network-based Model Reference Adaptive Control (DMRAC). We evaluate this new neuro-adaptive control architecture through flight tests on a small quadcopter. We demonstrate that a single DMRAC controller can handle significant nonlinearities due to severe system faults and deliberate wind disturbances while executing high-bandwidth attitude control. We also show that the architecture has long-term learning abilities across different flight regimes, and can generalize to fly different flight trajectories than those on which it was trained. These results demonstrating the efficacy of this architecture for high bandwidth closed-loop attitude control of unstable and nonlinear robots operating in adverse situations. To achieve these results, we designed a software+communication architecture to ensure online real-time inference of the deep network on a high-bandwidth computation-limited platform. We expect that this architecture will benefit other deep learning in the closed-loop experiments on robots.},    
    paperid  =  {206}    
}    

@InProceedings{veer20,    
    title  =  {Probably Approximately Correct Vision-Based Planning using Motion Primitives},    
    author  =  {Veer, Sushant and Majumdar, Anirudha},    
    pages  =  {1001--1014},     
    abstract  =  {This paper presents an approach for learning vision-based planners that provably generalize to novel environments (i.e., environments unseen during training). We leverage the Probably Approximately Correct (PAC)-Bayes framework to obtain an upper bound on the expected cost of policies across all environments. Minimizing the PAC-Bayes upper bound thus trains policies that are accompanied by a certificate of performance on novel environments. The training pipeline we propose provides strong generalization guarantees for deep neural network policies by (a) obtaining a good prior distribution on the space of policies using Evolutionary Strategies (ES) followed by (b) formulating the PAC-Bayes optimization as an efficiently-solvable parametric convex optimization problem. We demonstrate the efficacy of our approach for producing strong generalization guarantees for learned vision-based motion planners through two simulated examples: (1) an Unmanned Aerial Vehicle (UAV) navigating obstacle fields with an onboard vision sensor, and (2) a dynamic quadrupedal robot traversing rough terrains with proprioceptive and exteroceptive sensors.},    
    paperid  =  {211}    
}    

@InProceedings{villalonga20,    
    title  =  {Tactile Object Pose Estimation from the First Touch with Geometric Contact Rendering},    
    author  =  {Villalonga, Maria Bauza and Rodriguez, Alberto and Lim, Bryan and Valls, Eric and Sechopoulos, Theo},    
    pages  =  {1015--1029},     
    abstract  =  {In this paper, we present an approach to tactile pose estimation from the first touch for known objects. First, we create an object-agnostic map from real tactile observations to contact shapes. Next, for a new object with known geometry, we learn a tailored perception model completely in simulation. To do so, we simulate the contact shapes that a dense set of object poses would produce on the sensor. Then, given a new contact shape obtained from the sensor output, we match it against the pre-computed set using the object-specific embedding learned purely in simulation using contrastive learning. This results in a perception model that can localize objects from a single tactile observation. It also allows reasoning over pose distributions and including additional pose constraints coming from other perception systems or multiple contacts. We provide quantitative results for four objects. Our approach provides high accuracy pose estimations from distinctive tactile observations while regressing pose distributions to account for those contact shapes that could result from different object poses. We further extend and test our approach in multi-contact scenarios where several tactile sensors are simultaneously in contact with the object.},    
    paperid  =  {213}    
}    

@InProceedings{wang20e,    
    title  =  {ROLL: Visual Self-Supervised Reinforcement Learning with Object Reasoning},    
    author  =  {Wang, Yufei and Gautham, Narasimhan and Lin, Xingyu and Okorn, Brian and Held, David},    
    pages  =  {1030--1048},     
    abstract  =  {Current image-based reinforcement learning (RL) algorithms typically operate on the whole image without performing object-level reasoning.  This leads to inefficient goal sampling and ineffective reward functions. In this paper, we improve upon previous visual self-supervised RL by incorporating object-level reasoning and occlusion reasoning. Specifically, we use unknown object segmentation to ignore distractors in the scene for better reward computation and goal generation; we further enable occlusion reasoning by employing a novel auxiliary loss and training scheme. We demonstrate that our proposed algorithm, ROLL (Reinforcement learning with Object Level Learning), learns dramatically faster and achieves better final performance compared with previous methods in several simulated visual control tasks. Project video and code are available at https://sites.google.com/andrew.cmu.edu/roll.},    
    paperid  =  {215}    
}    

@InProceedings{pinneri20,    
    title  =  {Sample-efficient Cross-Entropy Method for Real-time Planning},    
    author  =  {Pinneri, Cristina and Sawant, Shambhuraj and Blaes, Sebastian and Achterhold, Jan and Stueckler, Joerg and Rolinek, Michal and Martius, Georg},    
    pages  =  {1049--1065},     
    abstract  =  {Trajectory optimizers for model-based reinforcement learning, such as the  Cross-Entropy Method (CEM), can yield compelling results even in high-dimensional control tasks and sparse-reward environments. However, their sampling inefficiency prevents them from being used for real-time planning and control. We propose an improved version of the CEM algorithm for fast planning, with novel additions including temporally-correlated actions and memory, requiring 2.7-22x less samples and yielding a performance increase of 1.2-10x in high-dimensional control problems.},    
    paperid  =  {217}    
}    

@InProceedings{sandha20,    
    title  =  {Sim2Real Transfer for Deep Reinforcement Learning with Stochastic State Transition Delays},    
    author  =  {Sandha, Sandeep Singh and Garcia, Luis and Balaji, Bharathan and Anwar, Fatima and Srivastava, Mani},    
    pages  =  {1066--1083},     
    abstract  =  {Deep Reinforcement Learning (RL) has demonstrated to be useful for a wide variety of robotics applications. To address sample efficiency and safety during training, it is common to train Deep RL policies in a simulator and then deploy to the real world, a process called Sim2Real transfer.  For robotics applications, the deployment heterogeneities and runtime compute stochasticity results in variable timing characteristics of sensor sampling rates and end-to-end delays from sensing to actuation. Prior works have used the technique of domain randomization to enable the successful transfer of policies across domains having different state transition delays. We show that variation in sampling rates and policy execution time leads to degradation in Deep RL policy performance, and that domain randomization is insufficient to overcome this limitation. We propose the Time-in-State RL (TSRL) approach, which includes delays and sampling rate as additional agent observations at training time to improve the robustness of Deep RL policies. We demonstrate the efficacy of TSRL on HalfCheetah, Ant, and car robot in simulation and on a real robot using a 1/18th scale car.},    
    paperid  =  {219}    
}    

@InProceedings{hafner20,    
    title  =  {Towards General and Autonomous Learning of Core Skills: A Case Study in Locomotion},    
    author  =  {Hafner, Roland and Hertweck, Tim and Kloeppner, Philipp and Bloesch, Michael and Neunert, Michael and Wulfmeier, Markus and Tunyasuvunakool, Saran and Heess, Nicolas and Riedmiller, Martin},    
    pages  =  {1084--1099},     
    abstract  =  {Modern Reinforcement Learning (RL) algorithms promise to solve difficult motor control problems directly from raw sensory inputs. Their attraction is due in part to the fact that they can represent a general class of methods that allow to learn a solution with a reasonably set reward and minimal prior knowledge, even in situations where it is difficult or expensive for a human expert. For RL to truly make good on this promise, however, we need algorithms and learning setups that can work across a broad range of problems with minimal problem specific adjustments or engineering. In this paper, we study this idea of generality in the locomotion domain. We develop a learning framework that can learn sophisticated locomotion behaviour for a wide spectrum of legged robots, such as bipeds, tripeds, quadrupeds and hexapods, including wheeled variants. Our learning framework relies on a data-efficient, off-policy multi-task RL algorithm and a small set of reward functions that are semantically identical across robots. To underline the general applicability of the method, we keep the hyper-parameter settings and reward definitions constant across experiments and rely exclusively on on-board sensing. For nine different types of robots, including a real-world quadruped robot, we demonstrate that the same algorithm can rapidly learn diverse and reusable locomotion skills without any platform specific adjustments or additional instrumentation of the learning setup.},    
    paperid  =  {221}    
}    

@InProceedings{rabiee20,    
    title  =  {IV-SLAM: Introspective Vision for Simultaneous Localization and Mapping},    
    author  =  {Rabiee, Sadegh and Biswas, Joydeep},    
    pages  =  {1100--1109},     
    abstract  =  {Existing solutions to visual simultaneous localization and mapping (V-SLAM) assume that errors in feature extraction and matching are independent and identically distributed (i.i.d), but this assumption is known to not be true -- features extracted from low-contrast regions of images exhibit wider error distributions than features from sharp corners. Furthermore, V-SLAM algorithms are prone to catastrophic tracking failures when sensed images include challenging conditions such as specular reflections, lens flare, or shadows of dynamic objects. To address such failures, previous work has focused on building more robust visual frontends, to filter out challenging features. In this paper, we present introspective vision for SLAM (IV-SLAM), a fundamentally different approach for addressing these challenges. IV-SLAM explicitly models the noise process of reprojection errors from visual features to be context-dependent, and hence non-i.i.d. We introduce an autonomously supervised approach for IV-SLAM to collect training data to learn such a context-aware noise model. Using this learned noise model, IV-SLAM guides feature extraction to select more features from parts of the image that are likely to result in lower noise, and further incorporate the learned noise model into the joint maximum likelihood estimation, thus making it robust to the aforementioned types of errors. We present empirical results to demonstrate that IV-SLAM 1) is able to accurately predict sources of error in input images, 2) reduces tracking error compared to V-SLAM, and 3) increases the mean distance between tracking failures by more than 70\% on challenging real robot data compared to V-SLAM.},    
    paperid  =  {239}    
}    

@InProceedings{ha20c,    
    title  =  {Learning to Walk in the Real World with Minimal Human Effort},    
    author  =  {Ha, Sehoon and Xu, Peng and Tan, Zhenyu and Levine, Sergey and Tan, Jie},    
    pages  =  {1110--1120},     
    abstract  =  {Reliable and stable locomotion has been one of the most fundamental challenges for legged robots. Deep reinforcement learning (deep RL) has emerged as a promising method for developing such control policies autonomously. In this paper, we develop a system for learning legged locomotion policies with deep RL in the real world with minimal human effort. The key difficulties for on-robot learning systems are automatic data collection and safety. We overcome these two challenges by developing a multi-task learning procedure and a safety-constrained RL framework. We tested our system on the task of learning to walk on three different terrains: flat ground, a soft mattress, and a doormat with crevices. Our system can automatically and efficiently learn locomotion skills on a Minitaur robot with little human intervention.},    
    paperid  =  {245}    
}    

@InProceedings{cruz20,    
    title  =  {Generation of Realistic Images for Learning in Simulation using FeatureGAN},    
    author  =  {Cruz, Nicolas and Ruiz-del-Solar, Javier},    
    pages  =  {1121--1136},     
    abstract  =  {This paper presents FeatureGan, a methodology to train image translators (generators) using an unpaired image training set. FeatureGan is based on the use of Generative Adversarial Networks (GAN) and has three main novel components: (i) the use of a feature loss to ensure alignment between the input and the generated image, (ii) the use of a feature pyramid discriminator, which uses a tensor composed of features at different levels of abstraction generated by a pre-trained network, and (iii) the introduction of a per class loss to improve the results in the simulation-to-reality task. The main advantage of the proposed methodology when compared to classical approaches is a more stable training process, which includes a higher resilience to common GAN problems such as mode collapse, as well as better and more consistent results. FeatureGan is also fast to train, easy to replicate, and especially suited to be used in simulation-to-reality applications where the generated realistic images allow to close the visual simulation-to-reality gap. As a proof of concept, we show the application of the proposed methodology in soccer robotics, where realistic images are generated in a soccer robotics simulator, and robot and ball detectors are trained using these images and then tested in reality. The same methodology is used to generate realistic images from images rendered in a video game. The realistic images are then used to train a semantic segmentation network.},    
    paperid  =  {248}    
}    

@InProceedings{schiel20,    
    title  =  {Incremental learning of EMG-based control commands using Gaussian Processes},    
    author  =  {Schiel, Felix and Hagengruber, Annette and Vogel, J\"{o}rn and Triebel, Rudolph},    
    pages  =  {1137--1146},     
    abstract  =  {Myoelectric control is the process of controlling a prosthesis or an assistive robot by using electrical signals of the muscles. Pattern recognition in myoelectric control is a challenging field, since the underlying distribution of the signal is likely to change during the application. Covariate shifts, including changes of the arm position or different levels of muscular activation, often lead to significant instability of the control signal. This work tries to overcome these challenges by enhancing a myoelectric human machine interface through the use of the sparse Gaussian Process (sGP) approximation Variational Free Energy and by the introduction of a novel adaptive model based on an unsupervised incremental learning approach. The novel adaptive model integrates an interclass and intraclass distance to improve prediction stability under challenging conditions. Furthermore, it demonstrates the successful incorporation of incremental updates which is shown to lead to a significantly increased performance and higher  stability of the predictions in an online user study.},    
    paperid  =  {249}    
}    

@InProceedings{song20,    
    title  =  {Flightmare: A Flexible Quadrotor Simulator},    
    author  =  {Song, Yunlong and Naji, Selim and Kaufmann, Elia and Loquercio, Antonio and Scaramuzza, Davide},    
    pages  =  {1147--1157},     
    abstract  =  {State-of-the-art quadrotor simulators have a rigid and highly-specialized structure: either are they really fast, physically accurate, or photo-realistic. In this work, we propose a paradigm shift in the development of simulators: moving the trade-off between accuracy and speed from the developers to the end-users. We use this idea to develop a flexible quadrotor simulator: Flightmare. In this work, we propose a novel quadrotor simulator: Flightmare.  Flightmare is composed of two main components: a configurable rendering engine built on Unity and a flexible physics engine for dynamics simulation. Those two components are totally decoupled and can run independently of each other. This makes our simulator extremely fast: rendering achieves speeds of up to 230 Hz, while physics simulation of up to 200,000 Hz on a laptop. In addition, Flightmare comes with several desirable features: (i) a large multi-modal sensor suite, including an interface to extract the 3D point-cloud of the scene; (ii) an API for reinforcement learning which can simulate hundreds of quadrotors in parallel; and (iii) integration with a virtual-reality headset for interaction with the simulated environment. We demonstrate the flexibility of Flightmare by using it for two different robotic tasks: quadrotor control using deep reinforcement learning and collision-free path planning in a complex 3D environment.},    
    paperid  =  {250}    
}    

@InProceedings{chen20a,    
    title  =  {Hardware as Policy: Mechanical and Computational Co-Optimization using Deep Reinforcement Learning},    
    author  =  {Chen, Tianjian and He, Zhanpeng and Ciocarlie, Matei},    
    pages  =  {1158--1173},     
    abstract  =  {Deep Reinforcement Learning (RL) has shown great success in learning complex control policies for a variety of applications in robotics. However, in most such cases, the hardware of the robot has been considered immutable, modeled as part of the environment.  In this study, we explore the problem of learning hardware and control parameters together in a unified RL framework. To achieve this, we propose to model the robot body as a ``hardware policy'', analogous to and optimized jointly with its computational counterpart. We show that, by modeling such hardware policies as auto-differentiable computational graphs, the ensuing optimization problem can be solved efficiently by gradient-based algorithms from the Policy Optimization family.  We present two such design examples:  a toy mass-spring problem, and a real-world problem of designing an underactuated hand.  We compare our method against traditional co-optimization approaches, and also demonstrate its effectiveness by building a physical prototype based on the learned hardware parameters.  Videos and more details are available at https://roamlab.github.io/hwasp/.},    
    paperid  =  {256}    
}    

@InProceedings{frossard20,    
    title  =  {StrObe: Streaming Object Detection from LiDAR Packets},    
    author  =  {Frossard, Davi and Suo, Shun Da and Casas, Sergio and Tu, James and Urtasun, Raquel},    
    pages  =  {1174--1183},     
    abstract  =  {Many modern robotics systems employ LiDAR as their main sensing modality due to its geometrical richness. Rolling shutter LIDARs are particularly common, in which a sweep is built through the accumulation of points over an entire revolution of the sensor, thus producing a 360 point cloud of the scene. Modern perception algorithms wait for the full sweep to be built before processing the data, which introduces an additional latency of up to 100ms. As a consequence, by the time an output is produced, it no longer accurately reflects the state of the world. This poses a big challenge, as robotics applications require minimal reaction times, such that maneuvers can be quickly planned in the event of a safety-critical situation. In this paper we propose StrObe, a novel approach that minimizes latency by ingesting packets of LiDAR and emitting a stream of detections without waiting for the full sweep to be built. StrObe reuses computations from previous points and iteratively updates the spatial memory of the scene as new evidence comes in, resulting in latency reduced accurate perception. We demonstrate the effectiveness of our approach on a large scale dataset, showing that our approach far outperforms the state-of-the-art when latency is taken into account while still matching the performance in the traditional setting.},    
    paperid  =  {263}    
}    

@InProceedings{zhao20c,    
    title  =  {Towards Robotic Assembly by Predicting Robust, Precise and Task-oriented Grasps},    
    author  =  {Zhao, Jialiang and Troniak, Daniel and Kroemer, Oliver},    
    pages  =  {1184--1194},     
    abstract  =  {Robust task-oriented grasp planning is vital for autonomous robotic precision assembly tasks. Knowledge of the objects' geometry and preconditions of the target task should be incorporated when determining the proper grasp to execute. However, several factors contribute to the challenges of realizing these grasps such as noise when controlling the robot, unknown object properties, and difficulties modeling complex object-object interactions. We propose a method that decomposes this problem and optimizes for grasp robustness, precision, and task performance by learning three cascaded networks. We evaluate our method in simulation on three common assembly tasks: inserting gears onto pegs, aligning brackets into corners, and inserting shapes into slots. Our policies are trained using a curriculum based on large-scale self-supervised grasp simulations with procedurally generated objects. Finally, we evaluate the performance of the first two tasks with a real robot where our method achieves 4.28mm error for bracket insertion and 1.44mm error for gear insertion.},    
    paperid  =  {264}    
}    

@InProceedings{vadivelu20,    
    title  =  {Learning to Communicate and Correct Pose Errors},    
    author  =  {Vadivelu, Nicholas and Ren, Mengye and Tu, James and Wang, Jingkang and Urtasun, Raquel},    
    pages  =  {1195--1210},     
    abstract  =  {Learned communication makes multi-agent systems more effective by aggregating distributed information. However, it also exposes individual agents to the threat of erroneous messages they receive. In this paper, we study the setting proposed in V2VNet, where nearby self-driving vehicles jointly perform object detection and motion forecasting in a cooperative manner. Despite a huge performance boost when the agents solve the task together, the gain is quickly diminished in the presence of pose noise since the communication relies on spatial transformations. Hence, we propose a novel neural reasoning framework that learns to communicate, to estimate errors, and finally, to reach a consensus about those errors. Experiments confirm that our proposed framework significantly improves the robustness of multi-agent self-driving perception systems under realistic and severe localization noise.},    
    paperid  =  {266}    
}    

@InProceedings{haris20,    
    title  =  {Visual Localization and Mapping with Hybrid SFA},    
    author  =  {Haris, Muhammad and Franzius, Mathias and Bauer-Wersing, Ute and Karanam, Sai Krishna Kaushik},    
    pages  =  {1211--1220},     
    abstract  =  {Visual localization is a crucial requirement in mobile robotics, field and service robotics, and self-driving cars. Recently, unsupervised learning with Slow Feature Analysis (SFA) has shown to produce spatial representations that enable localization from holistic images. The approach is faster and much less complex than state-of-the-art monocular visual SLAM methods while achieving similar localization performance in small-scale environments. However, the holistic approach's performance drops significantly for highly complex, large-scale environments due to scene variations occurring during a training phase. Instead of using holistic images, an alternative is to perform localization relative to unique regions present in a scene. Therefore, in this paper, we add a new component to the SFA localization pipeline that leverages state-of-the-art CNN to identify unique image regions. Hence we propose a hybrid approach that first learns such regions with a pre-trained CNN and then uses SFA for unsupervised pose estimation relative to each region. We present the experimental results from an autonomous robot in two different outdoor environments of varying complexity and size. The experiments show the proposed hybrid approach outperforms holistic SFA w.r.t localization accuracy in both environments, but benefits are more pronounced in the large-scale environment.},    
    paperid  =  {267}    
}    

@InProceedings{murooka20,    
    title  =  {EXI-Net: EXplicitly/Implicitly Conditioned Network for Multiple Environment Sim-to-Real Transfer},    
    author  =  {Murooka, Takayuki and Hamaya, Masashi and Drigalski, Felix von and Tanaka, Kazutoshi and Ijiri, Yoshihisa},    
    pages  =  {1221--1230},     
    abstract  =  {Sim-to-real transfer is attractive for robot learning, as it avoids the high cost of collecting data with real robots, but transferring agents from simulation to the real world is challenging. Previous studies have presented promising methods to solve this problem, but they may fail when a wider range of dynamics has to be considered. In this study, we propose a network architecture with explicit and implicit dynamics parameters for sim-to-real transfer from multiple environments. Using this method, we can simultaneously estimate the dynamics of the real world and optimize the action in various kinds of environments. The core novelty lies in the simultaneous dynamics estimation and action optimization, as well as the use of explicit (physically quantifiable) and implicit (latent) dynamics parameters to condition the network input. We apply our method to the object pushing task and verify its effectiveness by comparing it with previous methods and real-world experiments.},    
    paperid  =  {268}    
}    

@InProceedings{boerdijk20,    
    title  =  {Self-Supervised Object-in-Gripper Segmentation from Robotic Motions},    
    author  =  {Boerdijk, Wout and Sundermeyer, Martin and Durner, Maximilian and Triebel, Rudolph},    
    pages  =  {1231--1245},     
    abstract  =  {Accurate object segmentation is a crucial task in the context of robotic manipulation. However, creating sufficient annotated training data for neural networks is particularly time consuming and often requires manual labeling. To this end, we propose a simple, yet robust solution for learning to segment unknown objects grasped by a robot. Specifically, we exploit motion and temporal cues in RGB video sequences. Using optical flow estimation we first learn to predict segmentation masks of our given manipulator. Then, these annotations are used in combination with motion cues to automatically distinguish between background, manipulator and unknown, grasped object. In contrast to existing systems our approach is fully self-supervised and independent of precise camera calibration, 3D models or potentially imperfect depth data. We perform a thorough comparison with alternative baselines and approaches from literature. The object masks and views are shown to be suitable training data for segmentation networks that generalize to novel environments and also allow for watertight 3D reconstruction.},    
    paperid  =  {275}    
}    

@InProceedings{zhao20d,    
    title  =  {MELD: Meta-Reinforcement Learning from Images via Latent State Models},    
    author  =  {Zhao, Zihao and Nagabandi, Anusha and Rakelly, Kate and Finn, Chelsea and Levine, Sergey},    
    pages  =  {1246--1261},     
    abstract  =  {Meta-reinforcement learning algorithms can enable autonomous agents, such as robots, to quickly acquire new behaviors by leveraging prior experience in a set of related training tasks. However, the onerous data requirements of meta-training compounded with the challenge of learning from sensory inputs such as images have made meta-RL challenging to apply to real robotic systems. Latent state models, which learn compact state representations from a sequence of observations, can accelerate representation learning from visual inputs. In this paper, we leverage the perspective of meta-learning as task inference to show that latent state models can {\em also} perform meta-learning given an appropriately defined observation space. Building on this insight, we develop meta-RL with latent dynamics (MELD), an algorithm for meta-RL from images that performs inference in a latent state model to quickly acquire new skills given observations and rewards. MELD outperforms prior meta-RL methods on several simulated image-based robotic control problems, and enables a real WidowX robotic arm to insert an Ethernet cable into new locations given a sparse task completion signal after only $8$ hours of real world meta-training. To our knowledge, MELD is the first meta-RL algorithm trained in a real-world robotic control setting from images.},    
    paperid  =  {278}    
}    

@InProceedings{chen20b,    
    title  =  {Learning from Suboptimal Demonstration via Self-Supervised Reward Regression},    
    author  =  {Chen, Letian and Paleja, Rohan and Gombolay, Matthew},    
    pages  =  {1262--1277},     
    abstract  =  {Learning from Demonstration (LfD) seeks to democratize robotics by enabling non-roboticist end-users to teach robots to perform a task by providing a human demonstration. However, modern LfD techniques, e.g. inverse reinforcement learning (IRL), assume users provide at least stochastically optimal demonstrations. This assumption fails to hold in most real-world scenarios. Recent attempts to learn from sub-optimal demonstration leverage pairwise rankings and following the Luce-Shepard rule. However, we show these approaches make incorrect assumptions and thus suffer from brittle, degraded performance. We overcome these limitations in developing a novel approach that bootstraps off suboptimal demonstrations to synthesize optimality-parameterized data to train an idealized reward function. We empirically validate we learn an idealized reward function with ~0.95 correlation with ground-truth reward versus  ~0.75 for prior work. We can then train policies achieving ~200\% improvement over the suboptimal demonstration and ~90\% improvement over prior work. We present a physical demonstration of teaching a robot a topspin strike in table tennis that achieves 32\% faster returns and 40\% more topspin than user demonstration.},    
    paperid  =  {281}    
}    

@InProceedings{lambert20,    
    title  =  {Stein Variational Model Predictive Control},    
    author  =  {Lambert, Alexander and Ramos, Fabio and Boots, Byron and Fox, Dieter and Fishman, Adam},    
    pages  =  {1278--1297},     
    abstract  =  {Decision making under uncertainty is critical to real-world, autonomous systems. Model Predictive Control (MPC) methods have demonstrated favorable performance in practice, but remain limited when dealing with complex probability distributions. In this paper, we propose a generalization of MPC that represents a multitude of solutions as posterior distributions. By casting MPC as a Bayesian inference problem, we employ variational methods for posterior computation, naturally encoding the complexity and multi-modality of the decision making problem. We propose a Stein variational gradient descent method to estimate the posterior over control parameters, given a cost function and a sequence of state observations. We show that this framework leads to successful planning in challenging, non-convex optimal control problems.},    
    paperid  =  {282}    
}    

@InProceedings{franzese20,    
    title  =  {Learning Interactively to Resolve Ambiguity in Reference Frame Selection},    
    author  =  {Franzese, Giovanni and Celemin, Carlos and Kober, Jens},    
    pages  =  {1298--1311},     
    abstract  =  {In Learning from Demonstrations, ambiguities can lead to bad generalization of the learned policy. This paper proposes a framework called Learning Interactively to Resolve Ambiguity (LIRA), that recognizes ambiguous situations, in which more than one action have similar probabilities, avoids a random action selection, and uses the human feedback for solving them. The aim is to improve the user experience, the learning performance and safety. LIRA is tested in the selection of the right goal of Movement Primitives (MP) out of a candidate list if multiple contradictory generalizations of the demonstration(s) are possible. The framework is validated on different pick and place operations on a Emika-Franka Robot. A user study showed a significant reduction on the task load of the user, compared to a system that does not allow interactive resolution of ambiguities.},    
    paperid  =  {283}    
}    

@InProceedings{chen20c,    
    title  =  {Learning Trajectories for Visual-Inertial System Calibration via Model-based Heuristic Deep Reinforcement Learning},    
    author  =  {Chen, Le and Ao, Yunke and Tschopp, Florian and Cramariuc, Andrei and Breyer, Michel and Chung, Jen Jen and Siegwart, Roland and Cadena, Cesar},    
    pages  =  {1312--1325},     
    abstract  =  {Visual-inertial systems rely on precise calibrations of both camera intrinsics and inter-sensor extrinsics, which typically require manually performing complex motions in front of a calibration target. In this work we present a novel approach to obtain favorable trajectories for visual-inertial system calibration, using model-based deep reinforcement learning. Our key contribution is to model the calibration process as a Markov decision process and then use model-based deep reinforcement learning with particle swarm optimization to establish a sequence of calibration trajectories to be performed by a robot arm. Our experiments show that while maintaining similar or shorter path lengths, the trajectories generated by our learned policy result in lower calibration errors compared to random or handcrafted trajectories. The code is publicly available.},    
    paperid  =  {285}    
}    

@InProceedings{mehta20,    
    title  =  {A User's Guide to Calibrating Robotic Simulators},    
    author  =  {Mehta, Bhairav and Handa, Ankur and Fox, Dieter and Ramos, Fabio},    
    pages  =  {1326--1340},     
    abstract  =  {Simulators are a critical component of modern robotics research. Strategies for both perception and decision making can be studied in simulation first, before deployed to real world systems, saving on time and costs. Despite significant progress on the development of sim-to-real algorithms, the analysis of  different methods is still conducted in an ad-hoc manner without a consistent set of tests and metrics for comparison. This paper intends to fill this gap and proposes a set of benchmarks and a framework for the study of various algorithms aimed to transfer models and policies learnt in simulation to the real world. We conduct experiments on a wide range of well known simulated environments to characterise and offer insights into the performance of different algorithms. Our analysis can be useful for practitioners working in this area and can help make informed choices about the behaviour and main properties of sim-to-real algorithms.},    
    paperid  =  {286}    
}    

@InProceedings{boffi20,    
    title  =  {Learning Stability Certificates from Data},    
    author  =  {Boffi, Nicholas and Tu, Stephen and Matni, Nikolai and Slotine, Jean-Jacques and Sindhwani, Vikas},    
    pages  =  {1341--1350},     
    abstract  =  {Many existing tools in nonlinear control theory for establishing stability or safety of a dynamical system can be distilled to the construction of a certificate function which guarantees a desired property. However, algorithms for synthesizing certificate functions typically require a closed-form analytical expression of the underlying dynamics, which rules out their use on many modern robotic platforms. To circumvent this issue, we develop algorithms for learning certificate functions only from trajectory data. We establish bounds on the generalization error -- the probability that a certificate will not certify a new, unseen trajectory -- when learning from trajectories, and we convert such generalization error bounds into global stability guarantees. We demonstrate empirically that certificates for complex dynamics can be efficiently learned, and that the learned certificates can be used for downstream tasks such as adaptive control.},    
    paperid  =  {290}    
}    

@InProceedings{lindemann20,    
    title  =  {Learning Hybrid Control Barrier Functions from Data},    
    author  =  {Lindemann, Lars and Hu, Haimin and Robey, Alexander and Zhang, Hanwen and Dimarogonas, Dimos and Tu, Stephen and Matni, Nikolai},    
    pages  =  {1351--1370},     
    abstract  =  {Motivated by the lack of systematic tools to obtain safe control laws for hybrid systems, we propose an optimization-based framework for learning certifiably safe control laws from data. In particular, we assume a setting in which the system dynamics are known and in which data exhibiting safe system behavior is available. We propose hybrid control barrier functions for hybrid systems as a means to synthesize safe control inputs. Based on this notion, we present an optimization-based framework to learn such hybrid control barrier functions from data. Importantly, we identify sufficient conditions on the data such that feasibility of the optimization problem ensures correctness of the learned hybrid control barrier functions, and hence the safety of the system. We illustrate our findings in two simulations studies, including a compass gait walker.},    
    paperid  =  {293}    
}    

@InProceedings{zhang20,    
    title  =  {Map-Adaptive Goal-Based Trajectory Prediction},    
    author  =  {Zhang, Lingyao and Su, Po-Hsun and Hoang, Jerrick and Haynes, Galen Clark and Marchetti-Bowick, Micol},    
    pages  =  {1371--1383},     
    abstract  =  {We present a new method for multi-modal, long-term vehicle trajectory prediction. Our approach relies on using lane centerlines captured in rich maps of the environment to generate a set of proposed goal paths for each vehicle. Using these paths -- which are generated at run time and therefore dynamically adapt to the scene -- as spatial anchors, we predict a set of goal-based trajectories along with a categorical distribution over the goals. This approach allows us to directly model the goal-directed behavior of traffic actors, which unlocks the potential for more accurate long-term prediction. Our experimental results on both a large-scale internal driving dataset and on the public nuScenes dataset show that our model outperforms state-of-the-art approaches for vehicle trajectory prediction over a 6-second horizon. We also empirically demonstrate that our model is better able to generalize to road scenes from a completely new city than existing methods.},    
    paperid  =  {296}    
}    

@InProceedings{banerjee20,    
    title  =  {The RobotSlang Benchmark: Dialog-guided Robot Localization and Navigation},    
    author  =  {Banerjee, Shurjo and Thomason, Jesse and Corso, Jason},    
    pages  =  {1384--1393},     
    abstract  =  {Autonomous robot systems for applications from search and rescue to assistive guidance should be able to engage in natural language dialog with people. To study such cooperative communication, we introduce Robot Simultaneous Localization and Mapping with Natural Language (RobotSlang), a benchmark of 169 natural language dialogs between a human Driver controlling a robot and a human Commander providing guidance towards navigation goals. In each trial, the pair first cooperates to localize the robot on a global map visible to the Commander, then the Driver follows Commander instructions to move the robot to a sequence of target objects. We introduce a Localization from Dialog History (LDH) and a Navigation from Dialog History (NDH) task where a learned agent is given dialog and visual observations from the robot platform as input and must localize in the global map or navigate towards the next target object, respectively. RobotSlang is comprised of nearly 5k utterances and over 1k minutes of robot camera and control streams. We present an initial model for the NDH task, and show that an agent trained in simulation can follow the RobotSlang dialog-based navigation instructions for controlling a physical robot platform. Code and data are available at https://umrobotslang.github.io/.},    
    paperid  =  {297}    
}    

@InProceedings{blumenkamp20,    
    title  =  {The Emergence of Adversarial Communication in Multi-Agent Reinforcement Learning},    
    author  =  {Blumenkamp, Jan and Prorok, Amanda},    
    pages  =  {1394--1414},     
    abstract  =  {Many real-world problems require the coordination of multiple autonomous agents. Recent work has shown the promise of Graph Neural Networks (GNNs) to learn explicit communication strategies that enable complex multi-agent coordination. These works use models of cooperative multi-agent systems whereby agents strive to achieve a shared global goal. When considering agents with self-interested local objectives, the standard design choice is to model these as separate learning systems (albeit sharing the same environment). Such a design choice, however, precludes the existence of a single, differentiable communication channel, and consequently prohibits the learning of inter-agent communication strategies. In this work, we address this gap by presenting a learning model that accommodates individual non-shared rewards and a differentiable communication channel that is common among all agents. We focus on the case where agents have self-interested objectives, and develop a learning algorithm that elicits the emergence of adversarial communications. We perform experiments on multi-agent coverage and path planning problems, and employ a post-hoc interpretability technique to visualize the messages that agents communicate to each other. We show how a single self-interested agent is capable of learning highly manipulative communication strategies that allows it to significantly outperform a cooperative team of agents.},    
    paperid  =  {306}    
}    

@InProceedings{zambelli20,    
    title  =  {Learning rich touch representations through cross-modal self-supervision},    
    author  =  {Zambelli, Martina and Aytar, Yusuf and Visin, Francesco and Zhou, Yuxiang and Hadsell, Raia},    
    pages  =  {1415--1425},     
    abstract  =  {The sense of touch is fundamental in several manipulation tasks, but rarely used in robot manipulation. In this work we tackle the problem of learning rich touch features from cross-modal self-supervision. We evaluate them identifying objects and their properties in a few-shot classification setting. Two new datasets are introduced using a simulated anthropomorphic robotic hand equipped with tactile sensors on both synthetic and daily life objects. Several self-supervised learning methods are benchmarked on these datasets, by evaluating few-shot classification on unseen objects and poses. Our experiments indicate that cross-modal self-supervision effectively improves touch representation, and in turn has great potential to enhance robot manipulation skills.},    
    paperid  =  {320}    
}    

@InProceedings{ren20,    
    title  =  {Generalization Guarantees for Imitation Learning},    
    author  =  {Ren, Allen and Veer, Sushant and Majumdar, Anirudha},    
    pages  =  {1426--1442},     
    abstract  =  {Control policies from imitation learning can often fail to generalize to novel environments due to imperfect demonstrations or the inability of imitation learning algorithms to accurately infer the expert's policies. In this paper, we present rigorous generalization guarantees for imitation learning by leveraging the Probably Approximately Correct (PAC)-Bayes framework to provide upper bounds on the expected cost of policies in novel environments. We propose a two-stage training method where a latent policy distribution is first embedded with multi-modal expert behavior using a conditional variational autoencoder, and then ``fine-tuned'' in new training environments to explicitly optimize the generalization bound. We demonstrate strong generalization bounds and their tightness relative to empirical performance in simulation for (i) grasping diverse mugs, (ii) planar pushing with visual feedback, and (iii) vision-based indoor navigation, as well as through hardware experiments for the two manipulation tasks.},    
    paperid  =  {322}    
}    

@InProceedings{ji20,    
    title  =  {Multi-Modal Anomaly Detection for Unstructured and Uncertain Environments},    
    author  =  {Ji, Tianchen and Vuppala, Sri Theja and Chowdhary, Girish and Driggs-Campbell, Katherine},    
    pages  =  {1443--1455},     
    abstract  =  {To achieve high-levels of autonomy, modern robots require the ability to detect and recover from anomalies and failures with minimal human supervision. Multi-modal sensor signals could provide more information for such anomaly detection tasks; however, the fusion of high-dimensional and heterogeneous sensor modalities remains a challenging problem. We propose a deep learning neural network: supervised variational autoencoder (SVAE), for failure identification in unstructured and uncertain environments. Our model leverages the representational power of VAE to extract robust features from high-dimensional inputs for supervised learning tasks. The training objective unifies the generative model and the discriminative model, thus making the learning a one-stage procedure. Our experiments on real field robot data demonstrate superior failure identification performance than baseline methods, and that our model learns interpretable representations.},    
    paperid  =  {324}    
}    

@InProceedings{pignat20,    
    title  =  {Generative adversarial training of product of policies for robust and adaptive movement primitives},    
    author  =  {Pignat, Emmanuel and Girgin, Hakan and Calinon, Sylvain},    
    pages  =  {1456--1470},     
    abstract  =  {In learning from demonstrations, many generative models of trajectories make simplifying assumptions of independence. Correctness is sacrificed in the name of tractability and speed of the learning phase. The ignored dependencies, which are often the kinematic and dynamic constraints of the system, are then only restored when synthesizing the motion, which introduces possibly heavy distortions. In this work, we propose to use those approximate trajectory distributions as close-to-optimal discriminators in the popular generative adversarial framework to stabilize and accelerate the learning procedure. The two problems of adaptability and robustness are addressed with our method. In order to adapt the motions to varying contexts, we propose to use a product of Gaussian policies defined in several parametrized task spaces. Robustness to perturbations and varying dynamics is ensured with the use of stochastic gradient descent and ensemble methods to learn the stochastic dynamics. Two experiments are performed on a 7-DoF manipulator to validate the approach.},    
    paperid  =  {329}    
}    

@InProceedings{holtz20,    
    title  =  {Robot Action Selection Learning via Layered Dimension Informed Program Synthesis},    
    author  =  {Holtz, Jarrett and Guha, Arjun and Biswas, Joydeep},    
    pages  =  {1471--1480},     
    abstract  =  {Abstract: Action selection policies (ASPs), used to compose low-level robot skills into complex high-level tasks are commonly represented as neural networks (NNs) in the state of the art. Such a paradigm, while very effective, suffers from a few key problems: 1) NNs are opaque to the user and hence not amenable to verification, 2) they require significant amounts of training data, and 3) they are hard to repair when the domain changes. We present two key insights about ASPs for robotics. First, ASPs need to reason about physically meaningful quantities derived from the state of the world, and second, there exists a layered structure for composing these policies. Leveraging these insights, we introduce layered dimension-informed program synthesis (LDIPS) - by reasoning about the physical dimensions of state variables, and dimensional constraints on operators, LDIPS directly synthesizes ASPs in a human-interpretable domain-specific language that is amenable to program repair. We present empirical results to demonstrate that LDIPS 1) can synthesize effective ASPs for robot soccer and autonomous driving domains, 2) requires two orders of magnitude fewer training examples than a comparable NN representation, and 3) can repair the synthesized ASPs with only a small number of corrections when transferring from simulation to real robots.},    
    paperid  =  {331}    
}    

@InProceedings{wang20f,    
    title  =  {Policy learning in SE(3) action spaces},    
    author  =  {Wang, Dian and Kohler, Colin and Platt, Robert},    
    pages  =  {1481--1497},     
    abstract  =  {In the spatial action representation, the action space spans the space of target poses for robot motion commands, i.e. SE(2) or SE(3). This approach has been used to solve challenging robotic manipulation problems and shows promise. However, the method is often limited to a three dimensional action space and short horizon tasks. This paper proposes ASRSE3, a new method for handling higher dimensional spatial action spaces that transforms an original MDP with high dimensional action space into a new MDP with reduced action space and augmented state space. We also propose SDQfD, a variation of DQfD designed for large action spaces. ASRSE3 and SDQfD are evaluated in the context of a set of challenging block construction tasks. We show that both methods outperform standard baselines and can be used in practice on real robotics systems.},    
    paperid  =  {340}    
}    

@InProceedings{agnew20,    
    title  =  {Amodal 3D Reconstruction for Robotic Manipulation via Stability and Connectivity},    
    author  =  {Agnew, William and Xie, Christopher and Walsman, Aaron and Murad, Octavian and Wang, Yubo and Domingos), Pedro and Srinivasa, Siddhartha},    
    pages  =  {1498--1508},     
    abstract  =  {Learning-based 3D object reconstruction enables single- or few-shot estimation of 3D object models. For robotics,  this holds the potential to allow model-based methods to rapidly adapt to novel objects and scenes. Existing 3D reconstruction techniques optimize for visual reconstruction fidelity, typically measured by chamfer distance or voxel IOU. We find that when applied to realistic, cluttered robotics environments, these systems produce reconstructions with low physical realism, resulting in poor task performance when used for model-based control.  We propose ARM, an amodal 3D reconstruction system that introduces (1) a stability prior over object shapes, (2) a connectivity prior, and (3) a multi-channel input representation that allows for reasoning over relationships between groups of objects.  By using these priors over the physical properties of objects, our system improves reconstruction quality not just by standard visual metrics, but also performance of model-based control on a variety of robotics manipulation tasks in challenging, cluttered environments.},    
    paperid  =  {345}    
}    

@InProceedings{surovik20,    
    title  =  {Learning an Expert Skill-Space for Replanning Dynamic Quadruped Locomotion over Obstacles},    
    author  =  {Surovik, David and Melon, Oliwier and Geisert, Mathieu and Fallon, Maurice and Havoutis, Ioannis},    
    pages  =  {1509--1518},     
    abstract  =  {Function approximators are increasingly being considered as a tool for generating robot motions that are temporally extended and express foresight about the scenario at hand. While these longer behaviors are often necessary or beneficial, they also induce multimodality in the decision space, which complicates the training of a regression model on expert data. Motivated by the problem of quadrupedal locomotion over obstacles, we apply an approach that disentangles modal variation from task-to-solution regression by using a conditional variational autoencoder. The resulting decoder is a regression model that outputs trajectories based on the task and a real-valued latent mode vector representing a style of behavior. With the task consisting of robot-relative descriptions of the state, the goal, and nearby obstacles, this model is suitable for receding-horizon generation of structured dynamic motion. We test this approach, along with a trajectory library baseline method, for producing sustained locomotion plans that use a generalized gait. Both options strongly bias planned footholds away from obstacle regions, while the multimodal regressor is far less susceptible to violating kinematic constraints. We conclude by identifying further prospective benefits of the continuous latent mode representation, along with targets for future integration into a hardware-deployable pipeline including perception and control.},    
    paperid  =  {346}    
}    

@InProceedings{sun20b,    
    title  =  {Learning Certified Control Using Contraction Metric},    
    author  =  {Sun, Dawei and Jha, Susmit and Fan, Chuchu},    
    pages  =  {1519--1539},     
    abstract  =  {In this paper, we solve the problem of finding a certified control policy that drives a robot from any given initial state and under any bounded disturbance to the desired reference trajectory, with guarantees on the convergence or bounds on the tracking error. Such a controller is crucial in safe motion planning. We leverage the advanced theory in Control Contraction Metric and design a learning framework based on neural networks to co-synthesize the contraction metric and the controller for control-affine systems. We further provide methods to validate the convergence and bounded error guarantees. We demonstrate the performance of our method using a suite of challenging robotic models, including models with learned dynamics as neural networks. We compare our approach with leading methods using sum-of-squares programming, reinforcement learning, and model predictive control. Results show that our methods indeed can handle a broader class of systems with less tracking error and faster execution speed. Code is available at https://github.com/sundw2014/C3M.},    
    paperid  =  {347}    
}    

@InProceedings{murali20,    
    title  =  {Same Object, Different Grasps: Data and Semantic Knowledge for Task-Oriented Grasping},    
    author  =  {Murali, Adithyavairavan and Liu, Weiyu and Marino, Kenneth and Chernova, Sonia and Gupta, Abhinav},    
    pages  =  {1540--1557},     
    abstract  =  {Despite the enormous progress and generalization in robotic grasping in recent years, existing methods have yet to scale and generalize task-oriented grasping to the same extent. This is largely due to the scale of the datasets both in terms of the number of objects and tasks studied. We address these concerns with the TaskGrasp dataset which is more diverse both in terms of objects and tasks, and an order of magnitude larger than previous datasets. The dataset contains 250K task-oriented grasps for 56 tasks and 191 objects along with their RGB-D information. We take advantage of this new breadth and diversity in the data and present the GCNGrasp framework which uses the semantic knowledge of objects and tasks encoded in a knowledge graph to generalize to new object instances, classes and even new tasks. Our framework shows a significant improvement of around 12\% on held-out settings compared to baseline methods which do not use semantics. We demonstrate that our dataset and model are applicable for the real world by executing task-oriented grasps on a real robot on unknown objects. Code, data and supplementary video could be found at https://github.com/adithyamurali/TaskGrasp.},    
    paperid  =  {348}    
}    

@InProceedings{tanwani20,    
    title  =  {DIRL: Domain-Invariant Representation Learning for Sim-to-Real Transfer},    
    author  =  {Tanwani, Ajay},    
    pages  =  {1558--1571},     
    abstract  =  {Generating large-scale synthetic data in simulation is a feasible alternative to collecting/labelling real data for training vision-based deep learning models, albeit the modelling inaccuracies do not generalize to the physical world. In this paper, we present a domain-invariant representation learning (DIRL) algorithm to adapt deep models to the physical environment with a small amount of real data. Existing approaches that only mitigate the covariate shift by aligning the marginal distributions across the domains and assume the conditional distributions to be domain-invariant can lead to ambiguous transfer in real scenarios. We propose to jointly align the marginal (input domains) and the conditional (output labels) distributions to mitigate the covariate and the conditional shift across the domains with adversarial learning, and combine it with a triplet distribution loss to make the conditional distributions disjoint in the shared feature space. Experiments on digit domains yield state-of-the-art performance on challenging benchmarks, while sim-to-real transfer of object recognition for vision-based decluttering with a mobile robot improves from $26.8 \%$ to $91.0 \%$, resulting in $86.5 \%$ grasping accuracy of a wide variety of objects. Code and supplementary details are available at: https://sites.google.com/view/dirl},    
    paperid  =  {350}    
}    

@InProceedings{chen20d,    
    title  =  {Learning Hierarchical Task Networks with Preferences from Unannotated Demonstrations},    
    author  =  {Chen, Kevin and Srikanth, Nithin Shrivatsav and Kent, David and Ravichandar, Harish and Chernova, Sonia},    
    pages  =  {1572--1581},     
    abstract  =  {We address the problem of learning Hierarchical Task Networks (HTNs) from unannotated task demonstrations, while retaining action execution preferences present in the demonstration data.  We show that the problem of learning a complex HTN structure can be made analogous to the problem of series/parallel reduction of resistor networks, a foundational concept in Electrical Engineering.  Based on this finding, we present the CircuitHTN algorithm, which constructs an action graph representing the demonstrations, and then reduces the graph following rules for reducing combination electrical circuits. Evaluation on real-world household kitchen tasks shows that CircuitHTN outperforms prior work in task structure and preference learning, successfully handling large data sets and exhibiting similar action selection preferences as the demonstrations.},    
    paperid  =  {351}    
}    

@InProceedings{simeonov20,    
    title  =  {A Long Horizon Planning Framework for Manipulating Rigid Pointcloud Objects},    
    author  =  {Simeonov, Anthony and Du, Yilun and Kim, Beomjoon and Hogan, Francois and Tenenbaum, Joshua and Agrawal, Pulkit and Rodriguez, Alberto},    
    pages  =  {1582--1601},     
    abstract  =  {We present a framework for solving long-horizon planning problems involving manipulation of rigid objects that operates directly from a point-cloud observation. Our method plans in the space of object subgoals and frees the planner from reasoning about robot-object interaction dynamics. We show that for rigid-bodies, this abstraction can be realized using low-level manipulation skills that maintain sticking-contact with the object and represent subgoals as 3D transformations. To enable generalization to unseen objects and improve planning performance, we propose a novel way of representing subgoals for rigid-body manipulation and a graph-attention based neural network architecture for processing point-cloud inputs. We experimentally validate these choices using simulated and real-world experiments on the YuMi robot. Results demonstrate that our method can successfully manipulate new objects into target configurations requiring long-term planning. Overall, our framework realizes the best of the worlds of task-and-motion planning (TAMP) and learning-based approaches. Project website: https://anthonysimeonov.github.io/rpo-planning-framework/.},    
    paperid  =  {358}    
}    

@InProceedings{breyer20,    
    title  =  {Volumetric Grasping Network: Real-time 6 DOF Grasp Detection in Clutter},    
    author  =  {Breyer, Michel and Chung, Jen Jen and Ott, Lionel and Siegwart, Roland and Nieto, Juan},    
    pages  =  {1602--1611},     
    abstract  =  {General robot grasping in clutter requires the ability to synthesize grasps that work for previously unseen objects and that are also robust to physical interactions, such as collisions with other objects in the scene. In this work, we design and train a network that predicts 6 DOF grasps from 3D scene information gathered from an on-board sensor such as a wrist-mounted depth camera. Our proposed Volumetric Grasping Network (VGN) accepts a Truncated Signed Distance Function (TSDF) representation of the scene and directly outputs the predicted grasp quality and the associated gripper orientation and opening width for each voxel in the queried 3D volume. We show that our approach can plan grasps in only 10 ms and is able to clear 92\% of the objects in real-world clutter removal experiments without the need for explicit collision checking. The real-time capability opens up the possibility for closed-loop grasp planning, allowing robots to handle disturbances, recover from errors and provide increased robustness.},    
    paperid  =  {359}    
}    

@InProceedings{chou20,    
    title  =  {Uncertainty-Aware Constraint Learning for Adaptive Safe Motion Planning from Demonstrations},    
    author  =  {Chou, Glen and Berenson, Dmitry and Ozay, Necmiye},    
    pages  =  {1612--1639},     
    abstract  =  {We present a method for learning to satisfy uncertain constraints from demonstrations. Our method uses robust optimization to obtain a belief over the potentially infinite set of possible constraints consistent with the demonstrations, and then uses this belief to plan trajectories that trade off performance with satisfying the possible constraints. We use these trajectories in a closed-loop policy that executes and replans using belief updates, which incorporate data gathered during execution. We derive guarantees on the accuracy of our constraint belief and probabilistic guarantees on plan safety. We present results on a 7-DOF arm and 12D quadrotor, showing our method can learn to satisfy high-dimensional (up to 30D) uncertain constraints and outperforms baselines in safety and efficiency.},    
    paperid  =  {361}    
}    

@InProceedings{nguyen20a,    
    title  =  {Belief-Grounded Networks for Accelerated Robot Learning under Partial Observability},    
    author  =  {Nguyen, Hai and Daley, Brett and Song, Xinchao and Amato, Christopher and Platt, Robert},    
    pages  =  {1640--1653},     
    abstract  =  {Many important robotics problems are partially observable where a single visual or force-feedback measurement is insufficient to reconstruct the state. Standard approaches involve learning a policy over beliefs or observation-action histories.    However, both of these have drawbacks; it is expensive to track the belief online, and it is hard to learn policies directly over histories. We propose a method for policy learning under partial observability called the Belief-Grounded Network (BGN) in which an auxiliary belief-reconstruction loss incentivizes a neural network to concisely summarize its input history. Since the resulting policy is a function of the history rather than the belief, it can be executed easily at runtime. We compare BGN against several baselines on classic benchmark tasks as well as three novel robotic force-feedback tasks. BGN outperforms all other tested methods and its learned policies work well when transferred onto a physical robot.},    
    paperid  =  {364}    
}    

@InProceedings{duckworth20,    
    title  =  {Time-Bounded Mission Planning in Time-Varying Domains with Semi-MDPs and Gaussian Processes},    
    author  =  {Duckworth, Paul and Lacerda, Bruno and Hawes, Nick},    
    pages  =  {1654--1668},     
    abstract  =  {Uncertain, time-varying dynamic environments are ubiquitous in real world robotics. We propose an online planning framework to address time-bounded missions under time-varying dynamics, where those dynamics affect the duration and outcome of actions. We pose such problems as semi-Markov decision processes, where actions have a duration distributed according to an a priori unknown time-varying function. Our approach maintains a belief over this function, and time is propagated through a discrete search tree that efficiently maintains a subset of reachable states. We show improved mission performance on a marine vehicle simulator acting under real-world spatio-temporal ocean currents, and demonstrate the ability to solve co-safe linear temporal logic problems, which are more complex than the reachability problems tackled in previous approaches.},    
    paperid  =  {368}    
}    

@InProceedings{tung20,    
    title  =  {3D-OES: Viewpoint-Invariant Object-Factorized Environment Simulators},    
    author  =  {Tung, Hsiao-Yu and Xian, Zhou and Prabhudesai, Mihir and Lal, Shamit and Fragkiadaki, Katerina},    
    pages  =  {1669--1683},     
    abstract  =  {We propose an action-conditioned dynamics model that predicts scene changes caused by object and agent interactions in a viewpoint-invariant 3D neural scene representation space, inferred from RGB-D videos. In this 3D feature space, objects do not interfere with one another and their appearance persists over time and across viewpoints. This permits our model to predict future scenes long in the future by simply ``moving'' 3D object features based on cumulative object motion predictions.  Object motion predictions are computed by a graph neural network that operates over the object features extracted from the 3D neural scene representation. Our model's simulations can be decoded by a neural renderer into 2D image views from any desired viewpoint, which aids the interpretability of our latent 3D simulation space.  We show our model generalizes well its predictions across varying number and appearances of interacting objects as well as across camera viewpoints, outperforming existing 2D and 3D dynamics models. We further demonstrate sim-to-real transfer of the learnt dynamics by applying our model trained solely in simulation to model-based control for pushing objects to desired locations under clutter on a real robotic setup.},    
    paperid  =  {374}    
}    

@InProceedings{wahid20,    
    title  =  {Learning Object-conditioned Exploration using Distributed Soft Actor Critic},    
    author  =  {Wahid, Ayzaan and Stone, Austin and Chen, Kevin and Ichter, Brian and Toshev, Alexander},    
    pages  =  {1684--1695},     
    abstract  =  {Object navigation is defined as navigating to an object of a given label in a complex, unexplored environment. In its general form, this problem poses several challenges for Robotics: semantic exploration of unknown environments in search of an object and low-level control. In this work we study object-guided exploration and low-level control, and present an end-to-end trained navigation policy achieving a success rate of 0.68 and SPL of 0.58 on unseen, visually complex scans of real homes. We propose a highly scalable implementation of an off-policy Reinforcement Learning algorithm, distributed Soft Actor Critic, which allows the system to utilize 98M experience steps in 24 hours on 8 GPUs. Our system learns to control a differential drive mobile base in simulation from a stack of high dimensional observations commonly used on robotic platforms. The learned policy is capable of object-guided exploratory behaviors and low-level control learned from pure experiences in realistic environments.},    
    paperid  =  {381}    
}    

@InProceedings{haugaard20,    
    title  =  {Fast robust peg-in-hole insertion with continuous visual servoing},    
    author  =  {Haugaard, Rasmus and Langaa, Jeppe and Sloth, Christoffer and Buch, Anders},    
    pages  =  {1696--1705},     
    abstract  =  {This paper demonstrates a visual servoing method which is robust towards uncertainties related to system calibration and grasping, while significantly reducing the peg-in-hole time compared to classical methods and recent attempts based on deep learning. The proposed visual servoing method is based on peg and hole point estimates from a deep neural network in a multi-cam setup, where the model is trained on purely synthetic data. Empirical results show that the learnt model generalizes to the real world, allowing for higher success rates and lower cycle times than existing approaches.},    
    paperid  =  {384}    
}    

@InProceedings{wang20g,    
    title  =  {Learning a natural-language to LTL executable semantic parser for grounded robotics},    
    author  =  {Wang, Christopher and Ross, Candace and Kuo, Yen-Ling and Katz, Boris and Barbu, Andrei},    
    pages  =  {1706--1718},     
    abstract  =  {Children acquire their native language with apparent ease by observing how language is used in context and attempting to use it themselves. They do so without laborious annotations, negative examples, or even direct corrections. We take a step toward robots that can do the same by training a grounded semantic parser, which discovers latent linguistic representations that can be used for the execution of  natural-language commands. In particular, we focus on the difficult domain of commands with a temporal aspect, whose semantics we capture with Linear Temporal Logic, LTL. Our parser is trained with pairs of sentences and executions as well as an executor. At training time, the parser hypothesizes a meaning representation for the input as a formula in LTL. Three competing pressures allow the parser to discover  meaning from language. First, any hypothesized meaning for a sentence must be permissive enough to reflect all the annotated execution trajectories. Second, the executor --- a pretrained end-to-end LTL planner --- must find that the observed trajectories are  likely executions of the meaning. Finally, a generator, which reconstructs the original input, encourages the model to find representations that conserve knowledge about the command. Together these ensure that the meaning is neither too general nor too specific. Our model generalizes well, being able to parse and execute both machine-generated and human-generated commands, with near-equal accuracy, despite the fact that the human-generated sentences are much more varied and complex with an open lexicon. The approach presented here is not specific to LTL: it can be applied to any domain where sentence meanings can be hypothesized and  an executor can verify these meanings, thus opening the door to many applications for robotic agents.},    
    paperid  =  {385}    
}    

@InProceedings{zhou20b,    
    title  =  {PLAS: Latent Action Space for Offline Reinforcement Learning},    
    author  =  {Zhou, Wenxuan and Bajracharya, Sujay and Held, David},    
    pages  =  {1719--1735},     
    abstract  =  {The goal of offline reinforcement learning is to learn a policy from a fixed dataset, without further interactions with the environment. This setting will be an increasingly more important paradigm for real-world applications of reinforcement learning such as robotics, in which data collection is slow and potentially dangerous. Existing off-policy algorithms have limited performance on static datasets due to extrapolation errors from out-of-distribution actions. This leads to the challenge of constraining the policy to select actions within the support of the dataset during training. We propose to simply learn the Policy in the Latent Action Space (PLAS) such that this requirement is naturally satisfied. We evaluate our method on continuous control benchmarks in simulation and a deformable object manipulation task with a physical robot. We demonstrate that our method provides competitive performance consistently across various continuous control tasks and different types of datasets, outperforming existing offline reinforcement learning methods with explicit constraints.},    
    paperid  =  {386}    
}    

@InProceedings{kasper20,    
    title  =  {Unsupervised Metric Relocalization Using Transform Consistency Loss},    
    author  =  {Kasper, Mike and Nobre, Fernando and Heckman, Christoffer and Keivan, Nima},    
    pages  =  {1736--1745},     
    abstract  =  {Training networks to perform metric relocalization traditionally requires accurate image correspondences. In practice, these are obtained by restricting domain coverage, employing additional sensors, or capturing large multi-view datasets. We instead propose a self-supervised solution, which exploits a key insight: localizing a query image within a map should yield the same absolute pose, regardless of the reference image used for registration. Guided by this intuition, we derive a novel transform consistency loss. Using this loss function, we train a deep neural network to infer dense feature and saliency maps to perform robust metric relocalization in dynamic environments.  We evaluate our framework on synthetic and real-world data, showing our approach outperforms other supervised methods when a limited amount of ground-truth information is available.},    
    paperid  =  {389}    
}    

@InProceedings{achermann20,    
    title  =  {MultiPoint: Cross-spectral registration of thermal and optical aerial imagery},    
    author  =  {Achermann, Florian and Kolobov, Andrey and Dey, Debadeepta and Hinzmann, Timo and Chung, Jen Jen and Siegwart, Roland and Lawrance, Nicholas},    
    pages  =  {1746--1760},     
    abstract  =  {While optical cameras are ubiquitous in robotics, some robots can sense the world in several sections of the electromagnetic spectrum simultaneously, which can extend their capabilities in fundamental ways. For instance, many fixed-wing UAVs carry both optical and thermal imaging cameras, potentially allowing them to detect temperature difference-induced atmospheric updrafts, map their locations, and adjust their flight path accordingly to increase their time aloft. A key step for unlocking the potential offered by multi-spectral data is generating consistent, multi-spectral maps of the environment. In this work, we introduce MultiPoint, a novel data-driven method for generating interest points and associated descriptors for registering optical and thermal image pairs without knowledge of the relative camera viewpoints. Existing pixel-based alignment methods are accurate but too slow to work in near-real time, while feature-based methods such as SuperPoint are fast but produce poor-quality cross-spectral matches due to interest point instability in thermal images. MultiPoint capitalizes on the strengths of both approaches. An offline mutual information-based procedure is used to align cross-spectral image pairs from a training set, which are then processed by our generalized multi-spectral homographic adaptation stage to generate highly repeatable interest points that are invariant across viewpoint changes in both spectra. These are used to train a MultiPoint deep neural network by exposing  this model to both same-spectrum and cross-spectral image pairs. This model is then deployed for fast and accurate online interest point detection. We show that MultiPoint outperforms existing techniques for feature-based image alignment using a dataset of real-world thermal-optical imagery captured by a UAV during flights in different conditions and release this dataset, the first of its kind.},    
    paperid  =  {392}    
}    

@InProceedings{wang20h,    
    title  =  {TartanVO: A Generalizable Learning-based VO},    
    author  =  {Wang, Wenshan and Hu, Yaoyu and Scherer, Sebastian},    
    pages  =  {1761--1772},     
    abstract  =  {We present the first learning-based visual odometry (VO) model, which generalizes to multiple datasets and real-world scenarios and outperforms geometry-based methods in challenging scenes. We achieve this by leveraging the SLAM dataset TartanAir, which provides a large amount of diverse synthetic data in challenging environments. Furthermore, to make our VO model generalize across datasets, we propose an up-to-scale loss function and incorporate the camera intrinsic parameters into the model. Experiments show that a single model, TartanVO, trained only on synthetic data, without any finetuning, can be generalized to real-world datasets such as KITTI and EuRoC, demonstrating significant advantages over the geometry-based methods on challenging trajectories. Our code is available at https://github.com/castacks/tartanvo.},    
    paperid  =  {395}    
}    

@InProceedings{deng20,    
    title  =  {Soft Multicopter Control Using Neural Dynamics Identification},    
    author  =  {Deng, Yitong and Zhang, Yaorui and He, Xingzhe and Yang, Shuqi and Tong, Yunjin and Zhang, Michael and DiPietro, Daniel and Zhu, Bo},    
    pages  =  {1773--1782},     
    abstract  =  {We propose a data-driven method to automatically generate feedback controllers for soft multicopters featuring deformable materials, non-conventional geometries, and asymmetric rotor layouts, to deliver compliant deformation and agile locomotion. Our approach coordinates two sub-systems: a physics-inspired network ensemble that simulates the soft drone dynamics and a custom LQR control loop enhanced by a novel online-relinearization scheme to control the neural dynamics. Harnessing the insights from deformation mechanics, we design a decomposed state formulation whose modularity and compactness facilitate the dynamics learning while its measurability readies it for real-world adaptation. Our method is painless to implement, and requires only conventional, low-cost gadgets for fabrication. In a high-fidelity simulation environment, we demonstrate the efficacy of our approach by controlling a variety of customized soft multicopters to perform hovering, target reaching, velocity tracking, and active deformation.},    
    paperid  =  {396}    
}    

@InProceedings{pereira20,    
    title  =  {Safe Optimal Control Using Stochastic Barrier Functions and Deep Forward-Backward SDEs},    
    author  =  {Pereira, Marcus and Wang, Ziyi and Exarchos, Ioannis and Theodorou, Evangelos},    
    pages  =  {1783--1801},     
    abstract  =  {This paper introduces a new formulation for stochastic optimal control and stochastic  dynamic  optimization that ensures safety with respect to state and control constraints.  The proposed  methodology brings together concepts such as  Forward-Backward Stochastic Differential Equations,  Stochastic  Barrier Functions, Differentiable Convex Optimization and Deep Learning.  Using the aforementioned concepts, a Neural Network architecture is designed for safe trajectory optimization in which learning can be performed in  an end-to-end fashion. Simulations are performed on three systems to show the efficacy of the proposed methodology.},    
    paperid  =  {408}    
}    

@InProceedings{saund20,    
    title  =  {Diverse Plausible Shape Completions from Ambiguous Depth Images},    
    author  =  {Saund, Bradley and Berenson, Dmitry},    
    pages  =  {1802--1813},     
    abstract  =  {We propose PSSNet, a network architecture for generating diverse plausible 3D reconstructions from a single 2.5D depth image. Existing methods tend to produce only small variations on a single shape, even when multiple shapes are consistent with an observation. To obtain diversity we alter a Variational Auto Encoder by providing a learned shape bounding box feature as side information during training. Since these features are known during training, we are able to add a supervised loss to the encoder and noiseless values to the decoder. To evaluate, we sample a set of completions from a network, construct a set of plausible shape matches for each test observation, and compare using our plausible diversity metric defined over sets of shapes. We perform experiments using Shapenet mugs and partially-occluded YCB objects and find that our method performs comparably in datasets with little ambiguity, and outperforms existing methods when many shapes plausibly fit an observed depth image. We demonstrate one use for PSSNet on a physical robot when grasping objects in occlusion and clutter.},    
    paperid  =  {415}    
}    

@InProceedings{bhattacharya20,    
    title  =  {Multiagent Rollout and Policy Iteration for POMDP with Application to Multi-Robot Repair Problems},    
    author  =  {Bhattacharya, Sushmita and Kailas, Siva and Badyal, Sahil and Gil, Stephanie and Bertsekas, Dimitri},    
    pages  =  {1814--1828},     
    abstract  =  {In this paper we consider infinite horizon discounted dynamic programming problems with finite state and control spaces, partial state observations, and a multiagent structure. We discuss and compare algorithms that simultaneously or sequentially optimize the agents' controls by using multistep lookahead, truncated rollout with a known base policy, and a terminal cost function approximation. Our methods specifically address the computational challenges of partially observable multiagent problems. In particular: 1) We consider rollout algorithms that dramatically reduce required computation while preserving the key cost improvement property of the standard rollout method. The per-step computational requirements for our methods are on the order of $O(Cm)$ as compared with $O(C^m)$ for standard rollout, where $C$ is the maximum cardinality of the constraint set for the control component of each agent, and $m$ is the number of agents. 2) We show that our methods can be applied to challenging problems with a graph structure, including a class of robot repair problems whereby multiple robots collaboratively inspect and repair a system under partial information. 3) We provide a simulation study that compares our methods with existing methods, and demonstrate that our methods can handle larger and more complex partially observable multiagent problems (state space size $10^{37}$ and control space size $10^{7}$, respectively). In particular, we verify experimentally that our multiagent rollout methods perform nearly as well as standard rollout for problems with few agents, and produce satisfactory policies for problems with a larger number of agents that are intractable by standard rollout and other state of the art methods. Finally, we incorporate our multiagent rollout algorithms as building blocks in an approximate policy iteration scheme, where successive rollout policies are approximated by using neural network classifiers. While this scheme requires a strictly off-line implementation, it works well in our computational experiments and produces additional significant performance improvement over the single online rollout iteration method.},    
    paperid  =  {416}    
}    

@InProceedings{blukis20,    
    title  =  {Few-shot Object Grounding and Mapping for Natural Language Robot Instruction Following},    
    author  =  {Blukis, Valts and Knepper, Ross and Artzi, Yoav},    
    pages  =  {1829--1854},     
    abstract  =  {We study the problem of learning a robot policy to follow natural language instructions that can be easily extended to reason about new objects. We introduce a few-shot language-conditioned object grounding method trained from augmented reality data that uses exemplars to identify objects and align them to their mentions in instructions. We present a learned map representation that encodes object locations and their instructed use, and construct it from our few-shot grounding output. We integrate this mapping approach into an instruction-following policy, thereby allowing it to  reason about previously unseen objects at test-time by simply adding exemplars. We evaluate on the task of learning to map raw observations and instructions to continuous control of a physical quadcopter. Our approach significantly outperforms the prior state of the art in the presence of new objects, even when the prior approach observes all objects during training.},    
    paperid  =  {417}    
}    

@InProceedings{schwarting20,    
    title  =  {Deep Latent Competition: Learning to Race Using Visual Control Policies in Latent Space},    
    author  =  {Schwarting, Wilko and Seyde, Tim and Gilitschenski, Igor and Liebenwein, Lucas and Sander, Ryan and Karaman, Sertac and Rus, Daniela},    
    pages  =  {1855--1870},     
    abstract  =  {Learning competitive behaviors in multi-agent settings such as racing requires long-term reasoning about potential adversarial interactions. This paper presents Deep Latent Competition (DLC), a novel reinforcement learning algorithm that learns competitive visual control policies through self-play in imagination. The DLC agent imagines multi-agent interaction sequences in the compact latent space of a learned world model that combines a joint transition function with opponent viewpoint prediction. Imagined self-play reduces costly sample generation in the real world, while the latent representation enables planning to scale gracefully with observation dimensionality.  We demonstrate the effectiveness of our algorithm in learning competitive behaviors on a novel multi-agent racing benchmark that requires planning from image observations. Code and videos available at https://sites.google.com/view/deep-latent-competition.},    
    paperid  =  {420}    
}    

@InProceedings{wuthrich20,    
    title  =  {TriFinger: An Open-Source Robot for Learning Dexterity},    
    author  =  {Wuthrich, Manuel and Widmaier, Felix and Grimminger, Felix and Joshi, Shruti and Agrawal, Vaibhav and Hammoud, Bilal and Khadiv, Majid and Bogdanovic, Miroslav and Berenz, Vincent and Viereck, Julian and Naveau, Maximilien and Righetti, Ludovic and Sch\"{o}lkopf, Bernhard and Bauer, Stefan},    
    pages  =  {1871--1882},     
    abstract  =  {Dexterous object manipulation is still an open problem in robotics, despite the rapid progress in machine learning during the past decade. We argue that a key issue which has hindered progress is the high cost of experimentation on real systems, in terms of both time and money. We address this problem by proposing a novel open-source robotic platform, consisting of hardware and software, to drastically reduce the cost of experimentation. The hardware is inexpensive yet highly dynamic, robust, and capable of complex contact interaction with external objects. The software allows for 1-kilohertz real-time control and performs safety checks to prevent the hardware from breaking. These properties enable the platform to run without human supervision. In addition, we provide easy-to-use C++ and Python interfaces. We illustrate the potential of the proposed platform by performing an object-manipulation task using an optimal-control algorithm and training a learning-based method directly on the real system.},    
    paperid  =  {421}    
}    

@InProceedings{park20,    
    title  =  {Learning to Improve Multi-Robot Hallway Navigation},    
    author  =  {Park, Jin Soo and Tsang, Brian and Yedidsion, Harel and Warnell, Garrett and Kyoung, Daehyun and Stone, Peter},    
    pages  =  {1883--1895},     
    abstract  =  {As multi-robot applications become more prevalent, it becomes necessary to develop navigation systems which allow autonomous mobile robots to efficiently and safely pass each other in confined spaces. Existing navigation systems, such as the widely used ROS Navigation Stack, usually produce safe, collision free paths in static environments. However, these systems are not perfect, and when multiple mobile robots simultaneously navigate in narrow spaces, collisions and turnarounds are not uncommon. Fine-tuning and enhancing such navigation stacks is not as simple as it looks since they are made up of multiple layers of code, and there exists a tradeoff between optimizing for efficiency, i.e. minimizing time to destination (TTD) vs. optimizing for safety, i.e. minimizing collisions, with each objective leading to a different combination of parameter values. In this paper we develop a methodology to improve existing navigation stacks with regards to both objectives, without tuning their parameters, while preserving their inherent safety control properties. Our proposed approach is a decentralized learning-based approach that is geared toward real world robotic deployment, by requiring little computing resources. It is agnostic of the underlying navigation stack and can adapt to different types of environmental layouts (i.e., hallway structures).},    
    paperid  =  {424}    
}    

@InProceedings{akbulut20,    
    title  =  {ACNMP: Skill Transfer and Task Extrapolation through Learning from Demonstration and Reinforcement Learning via Representation Sharing},    
    author  =  {Akbulut, Mete and Oztop, Erhan and Seker, Muhammet Yunus and X, Hh and Tekden, Ahmet and Ugur, Emre},    
    pages  =  {1896--1907},     
    abstract  =  {To equip robots with dexterous skills, an effective approach is to first transfer the desired skill via Learning from Demonstration (LfD), then let the robot improve it by self-exploration via Reinforcement Learning (RL). In this paper, we propose a novel LfD+RL framework, namely Adaptive Conditional Neural Movement Primitives (ACNMP), that allows efficient policy improvement in novel environments and effective skill transfer between different agents. This is achieved through exploiting the latent representation learned by the underlying Conditional Neural Process (CNP) model, and simultaneous training of the model with supervised learning (SL) for acquiring the demonstrated trajectories and via RL for new trajectory discovery. Through simulation experiments, we show that (i) ACNMP enables the system to extrapolate to situations where pure LfD fails; (ii) Simultaneous training of the system through SL and RL  preserves the shape of demonstrations while adapting to novel situations due to the shared representations used by both learners; (iii) ACNMP enables order-of-magnitude sample-efficient RL in extrapolation of reaching tasks compared to the existing approaches; (iv) ACNMPs can be used to implement skill transfer between robots having different morphology, with competitive learning speeds and importantly with less number of assumptions compared to the state-of-the-art approaches. Finally, we show the real-world suitability of ACNMPs through real robot experiments that involve obstacle avoidance, pick and place and pouring actions.},    
    paperid  =  {426}    
}    

@InProceedings{li20a,    
    title  =  {Unsupervised Monocular Depth Learning in Dynamic Scenes},    
    author  =  {Li, Hanhan and Gordon, Ariel and Zhao, Hang and Casser, Vincent and Angelova, Anelia},    
    pages  =  {1908--1917},     
    abstract  =  {We present a method for jointly training the estimation of depth, ego-motion, and a dense 3D translation field of objects relative to the scene, with monocular photometric consistency being the sole source of supervision. We show that this apparently heavily underdetermined problem can be regularized by imposing the following prior knowledge about 3D translation fields: they are sparse, since most of the scene is static, and they tend to be piecewise constant for rigid moving objects. We show that this regularization alone is sufficient to train monocular depth prediction models that exceed the accuracy achieved in prior work for dynamic scenes, including methods that require semantic input.},    
    paperid  =  {427}    
}    

@InProceedings{jain20b,    
    title  =  {BayesRace: Learning to race autonomously using prior experience},    
    author  =  {Jain, Achin and O'Kelly, Matthew and Chaudhari, Pratik and Morari, Manfred},    
    pages  =  {1918--1929},     
    abstract  =  {Autonomous race cars require perception, estimation, planning, and control modules which work together asynchronously while driving at the limit of a vehicle's handling capability. A fundamental challenge encountered in designing these software components lies in predicting the vehicle's future state (e.g. position, orientation, and speed) with high accuracy. The root cause is the difficulty in identifying vehicle model parameters that capture the effects of lateral tire slip. We present a model-based planning and control framework for autonomous racing that significantly reduces the effort required in system identification and control design. Our approach alleviates the gap induced by simulation-based controller design by learning from on-board sensor measurements. A major focus of this work is empirical, thus, we demonstrate our contributions by experiments on validated 1:43 and 1:10 scale autonomous racing simulations.},    
    paperid  =  {428}    
}    

@InProceedings{das20,    
    title  =  {Model-Based Inverse Reinforcement Learning from Visual Demonstrations},    
    author  =  {Das, Neha and Bechtle, Sarah and Davchev, Todor and Jayaraman, Dinesh and Rai, Akshara and Meier, Franziska},    
    pages  =  {1930--1942},     
    abstract  =  {Scaling model-based inverse reinforcement learning (IRL) to real robotic manipulation tasks with unknown dynamics remains an open problem. The key challenges lie in learning good dynamics models, developing algorithms that scale to high-dimensional state-spaces and being able to learn from both visual and proprioceptive demonstrations. In this work, we present a gradient-based inverse reinforcement learning framework that utilizes a pre-trained visual dynamics model to learn cost functions when given only visual human demonstrations. The learned cost functions are then used to reproduce the demonstrated behavior via visual model predictive control. We evaluate our framework on hardware on two basic object manipulation tasks.},    
    paperid  =  {432}    
}    

@InProceedings{ota20,    
    title  =  {Deep Reactive Planning in Dynamic Environments},    
    author  =  {Ota, Kei and Jha, Devesh and Onishi, Tadashi and Kanezaki, Asako and Yoshiyasu, Yusuke and Sasaki, Yoko and Mariyama, Toshisada and Nikovski, Daniel},    
    pages  =  {1943--1957},     
    abstract  =  {The main novelty of the proposed approach is that it allows a robot to learn an end-to-end policy which can adapt to changes in the environment during execution. While goal conditioning of policies has been studied in the RL literature, such approaches are not easily extended to settings where the robot's goal can change during execution. This is something that humans are naturally able to do. However, it is difficult for robots to learn such reflexes (i.e., to naturally respond to dynamic environments), especially when the goal location is not explicitly provided to the robot, and instead needs to be perceived through a vision sensor. In the current work, we present a method that can achieve such behavior by combining traditional kinematic planning, deep learning, and deep reinforcement learning in a synergistic fashion to generalize to arbitrary environments. We demonstrate the proposed approach for several reaching and pick-and-place tasks in simulation, as well as on a real system of a 6-DoF industrial manipulator.},    
    paperid  =  {439}    
}    

@InProceedings{chen20e,    
    title  =  {Reactive motion planning with probabilisticsafety guarantees},    
    author  =  {Chen, Yuxiao and Rosolia, Ugo and Fan, Chuchu and Ames, Aaron and Murray, Richard},    
    pages  =  {1958--1970},     
    abstract  =  {Motion planning in environments with multiple agents is critical to many important autonomous applications such as autonomous vehicles and assistive robots. This paper considers the problem of motion planning, where the controlled agent shares the environment with multiple uncontrolled agents. First, a predictive model of the uncontrolled agents is trained to predict all possible trajectories within a short horizon based on the scenario. The prediction is then fed to a motion planning module based on model predictive control. We proved generalization bound for the predictive model using three different methods, post-bloating, support vector machine (SVM), and conformal analysis, all capable of generating stochastic guarantees of the correctness of the predictor. The proposed approach is demonstrated in simulation in a scenario emulating autonomous highway driving.},    
    paperid  =  {440}    
}    

@InProceedings{xu20d,    
    title  =  {Hierarchical Robot Navigation in Novel Environments using Rough 2-D Maps},    
    author  =  {Xu, Chengguang and Amato, Christopher and Wong, Lawson},    
    pages  =  {1971--1991},     
    abstract  =  {In robot navigation, generalizing quickly to unseen environments is essential.  Hierarchical methods inspired by human navigation have been proposed, typically consisting of a high-level landmark proposer and a low-level controller. However, these methods either require precise high-level information to be given in advance, or need to construct such guidance from extensive interaction with the environment. In this work, we propose an approach that leverages a rough 2-D map of the environment to navigate in novel environments without requiring further learning. In particular, we introduce a dynamic topological map that can be initialized from the rough 2-D map along with a high-level planning approach for proposing reachable 2-D map patches of the intermediate landmarks between the start and goal locations. To use proposed 2-D patches, we train a deep generative model to generate intermediate landmarks in observation space which are used as subgoals by low-level goal-conditioned reinforcement learning. Importantly, because the low-level controller is only trained with local behaviors (e.g. go across the intersection, turn left at a corner) on existing environments, this framework allows us to generalize to novel environments given only a rough 2-D map, without requiring further learning. Experimental results demonstrate the effectiveness of the proposed framework in both seen and novel environments.},    
    paperid  =  {442}    
}    

@InProceedings{young20,    
    title  =  {Visual Imitation Made Easy},    
    author  =  {Young, Sarah and Gandhi, Dhiraj and Tulsiani, Shubham and Gupta, Abhinav and Abbeel, Pieter and Pinto, Lerrel},    
    pages  =  {1992--2005},     
    abstract  =  {Visual imitation learning provides a framework for learning complex manipulation behaviors by leveraging human demonstrations. However, current interfaces for imitation such as kinesthetic teaching or teleoperation prohibitively restrict our ability to efficiently collect large-scale data in the wild. Obtaining such diverse demonstration data is paramount for the generalization of learned skills to novel scenarios. In this work, we present an alternate interface for imitation that simplifies the data collection process while allowing for easy transfer to robots. We use commercially available reacher-grabber assistive tools both as a data collection device and as the robot's end-effector. To extract action information from these visual demonstrations, we use off-the-shelf Structure from Motion (SfM) techniques in addition to training a finger detection network. We experimentally evaluate on two challenging tasks: non-prehensile pushing and prehensile stacking, with 1000 diverse demonstrations for each task. For both tasks, we use standard behavior cloning to learn executable policies from the previously collected offline demonstrations. To improve learning performance, we employ a variety of data augmentations and provide an extensive analysis of its effects. Finally, we demonstrate the utility of our interface by evaluating on real robotic scenarios with previously unseen objects and achieve a 87\% success rate on pushing and a 62\% success rate on stacking. Robot videos are available at our  project website: https://sites.google.com/view/visual-imitation-made-easy .},    
    paperid  =  {444}    
}    

@InProceedings{katara20,    
    title  =  {DeepMPCVS: Deep Model Predictive Control for Visual Servoing},    
    author  =  {Katara, Pushkal and YVS, Harish and Pandya, Harit and Gupta, Abhinav and Sanchawala, AadilMehdi and Kumar, Gourav and Bhowmick, Brojeshwar and Krishna, Madhava},    
    pages  =  {2006--2015},     
    abstract  =  {The simplicity of the visual servoing approach makes it an attractive option for tasks dealing with vision-based control of robots in many real-world applications. However, attaining precise alignment for unseen environments pose a challenge to existing visual servoing approaches. While classical approaches assume a perfect world, the recent data-driven approaches face issues when generalizing to novel environments. In this paper, we aim to combine the best of both worlds. We present a deep model predictive visual servoing framework that can achieve precise alignment with optimal trajectories and can generalize to novel environments. Our framework consists of a deep network for optical flow predictions, which are used along with a predictive model to forecast future optical flow. For generating an optimal set of velocities we present a control network that can be trained on-the-fly without any supervision. Through extensive simulations on photo-realistic indoor settings of the popular Habitat framework, we show significant performance gain due to the proposed formulation vis-a-vis recent state of the art methods. Specifically, we show vastly improved performance in trajectory length and faster convergence over recent approaches.},    
    paperid  =  {448}    
}    

@InProceedings{tang20a,    
    title  =  {Deep Reinforcement Learning with Population-Coded Spiking Neural Network for Continuous Control},    
    author  =  {Tang, Guangzhi and Kumar, Neelesh and Yoo, Raymond and Michmizos, Konstantinos},    
    pages  =  {2016--2029},     
    abstract  =  {The energy-efficient control of mobile robots has become crucial as the complexity of their real-world applications increasingly involves high-dimensional observation and action spaces, which cannot be offset by their limited on-board resources. An emerging non-Von Neumann model of intelligence, where spiking neural networks (SNNs) are executed on neuromorphic processors, is now considered as an energy-efficient and robust alternative to the state-of-the-art real-time robotic controllers for low dimensional control tasks. The challenge now for this new computing paradigm is to scale so that it can keep up with real-world applications. To do so, SNNs need to overcome the inherent limitations of their training, namely the limited ability of their spiking neurons to represent information and the lack of effective learning algorithms. Here, we propose a population-coded spiking actor network (PopSAN) that was trained in conjunction with a deep critic network using deep reinforcement learning (DRL). The population coding scheme, which is prevalent across brain networks, dramatically increased the representation capacity of the network and the hybrid learning combined the training advantages of deep networks with the energy-efficient inference of spiking networks. To show that our approach can be used for general-purpose spike-based reinforcement learning, we demonstrated its integration with a wide spectrum of policy-gradient based DRL methods covering both on-policy and off-policy DRL algorithms. We deployed the trained PopSAN on Intel's Loihi neuromorphic chip and benchmarked our method against the mainstream DRL algorithms for continuous control. To allow for a fair comparison among all methods, we validated them on OpenAI gym tasks. Our Loihi-run PopSAN consumed 140 times less energy per inference when compared against the deep actor network on Jetson TX2, and achieved the same level of performance. Our results demonstrate the overall efficiency of neuromorphic controllers and suggest the hybrid reinforcement learning approach as an alternative to deep learning, when both energy-efficiency and robustness are important.},    
    paperid  =  {450}    
}    

@InProceedings{niu20,    
    title  =  {Tolerance-Guided Policy Learning for Adaptable and Transferrable Delicate Industrial Insertion},    
    author  =  {Niu, Boshen and Wang, Chenxi and Liu, Changliu},    
    pages  =  {2030--2039},     
    abstract  =  {Policy learning for delicate industrial insertion tasks (e.g., PC board assembly) is challenging. This paper considers two major problems: how to learn a diversified policy (instead of just one average policy) that can efficiently handle different workpieces with minimum amount of training data, and how to handle defects of workpieces during insertion. To address the problems, we propose tolerance-guided policy learning. To encourage transferability of the learned policy to different workpieces, we add a task embedding to the policy's input space using the insertion tolerance. Then we train the policy using generative adversarial imitation learning with reward shaping (RS-GAIL) on a variety of representative situations. To encourage adaptability of the learned policy to handle defects, we build a probabilistic inference model that can output the best inserting pose based on failed insertions using the tolerance model. The best inserting pose is then used as a reference to the learned policy. This proposed method is validated on a sequence of IC socket insertion tasks in simulation. The results show that 1) RS-GAIL can efficiently learn optimal policies under sparse rewards; 2) the tolerance embedding can enhance the transferability of the learned policy; 3) the probabilistic inference makes the policy robust to defects on the workpieces.},    
    paperid  =  {452}    
}    

@InProceedings{aljalbout20,    
    title  =  {Learning Vision-based Reactive Policies for Obstacle Avoidance},    
    author  =  {Aljalbout, Elie and Chen, Ji and Ritt, Konstantin and Ulmer, Maximilian and Haddadin, Sami},    
    pages  =  {2040--2054},     
    abstract  =  {In this paper, we address the problem of vision-based obstacle avoidance for robotic manipulators. This topic poses challenges for both perception and motion generation. While most work in the field aims at improving one of those aspects, we provide a unified framework for approaching this problem. The main goal of this framework is to connect perception and motion by identifying the relationship between the visual input and the corresponding motion representation. To this end, we propose a method for learning reactive obstacle avoidance policies. We evaluate our method on goal-reaching tasks for single and multiple obstacles scenarios. We show the ability of the proposed method to efficiently learn stable obstacle avoidance strategies at a high success rate while maintaining closed-loop responsiveness required for critical applications like human-robot interaction.},    
    paperid  =  {454}    
}    

@InProceedings{lew20,    
    title  =  {Sampling-based Reachability Analysis: A Random Set Theory Approach with Adversarial Sampling},    
    author  =  {Lew, Thomas and Pavone, Marco},    
    pages  =  {2055--2070},     
    abstract  =  {Reachability analysis is at the core of many applications, from neural network verification, to safe trajectory planning of uncertain systems. However, this problem is notoriously challenging, and current approaches tend to be either too restrictive, too slow, too conservative, or approximate and therefore lack guarantees. In this paper, we propose a simple yet effective sampling-based approach to perform reachability analysis for arbitrary dynamical systems. Our key novel idea consists of using random set theory to give a rigorous interpretation of our method, and prove that it returns sets which are guaranteed to converge to the convex hull of the true reachable sets. Additionally, we leverage recent work on robust deep learning and propose a new adversarial sampling approach to robustify our algorithm and accelerate its convergence. We demonstrate that our method is faster and less conservative than prior work, present results for approximate reachability analysis of neural networks and robust trajectory optimization of high-dimensional uncertain nonlinear systems, and discuss future applications.},    
    paperid  =  {459}    
}    

@InProceedings{dasari20,    
    title  =  {Transformers for One-Shot Visual Imitation},    
    author  =  {Dasari, Sudeep and Gupta, Abhinav},    
    pages  =  {2071--2084},     
    abstract  =  {Humans are able to seamlessly visually imitate others, by inferring their intentions and using past experience to achieve the same end goal. In other words, we can parse complex semantic knowledge from raw video and efficiently translate that into concrete motor control. Is it possible to give a robot this same capability? Prior research in robot imitation learning has created agents which can acquire diverse skills from expert human operators. However, expanding these techniques to work with a single positive example during test time is still an open challenge. Apart from control, the difficulty stems from mismatches between the demonstrator and robot domains. For example, objects may be placed in different locations (e.g. kitchen layouts are different in every house). Additionally, the demonstration may come from an agent with different morphology and physical appearance (e.g. human), so one-to-one action correspondences are not available. This paper investigates techniques which allow robots to partially bridge these domain gaps, using their past experience. A neural network is trained to mimic ground truth robot actions given context video from another agent, and must generalize to unseen task instances when prompted with new videos during test time. We hypothesize that our policy representations must be both context driven and dynamics aware in order to perform these tasks. These assumptions are baked into the neural network using the Transformers attention mechanism and a self-supervised inverse dynamics loss. Finally, we experimentally determine that our method accomplishes a 2x improvement in terms of task success rate over prior baselines in a suite of one-shot manipulation tasks.},    
    paperid  =  {463}    
}    

@InProceedings{tang20b,    
    title  =  {Self-Supervised 3D Keypoint Learning for Ego-Motion Estimation},    
    author  =  {Tang, Jiexiong and Ambrus, Rares and Guizilini, Vitor and Pillai, Sudeep and Kim, Hanme and Jensfelt, Patric and Gaidon, Adrien},    
    pages  =  {2085--2103},     
    abstract  =  {Detecting and matching robust viewpoint-invariant keypoints is critical for visual SLAM and Structure-from-Motion. State-of-the-art learning-based methods generate training samples via homography adaptation to create 2D synthetic views with known keypoint matches from a single image. This approach does not, however, generalize to non-planar 3D scenes with illumination variations commonly seen in real-world videos. In this work, we propose self-supervised learning depth-aware keypoints from unlabeled videos directly. We jointly learn keypoint and depth estimation networks by combining appearance and geometric matching via a differentiable structure-from-motion module based on Procrustean residual pose correction. We show how our self-supervised keypoints can be trivially incorporated into state-of-the-art visual odometry frameworks for robust and accurate ego-motion estimation of autonomous vehicles in real-world conditions.},    
    paperid  =  {464}    
}    

@InProceedings{nguyen20b,    
    title  =  {Self-Supervised Learning of Scene-Graph Representations for Robotic Sequential Manipulation Planning},    
    author  =  {Nguyen, Son and Oguz, Ozgur and Hartmann, Valentin and Toussaint, Marc},    
    pages  =  {2104--2119},     
    abstract  =  {We present a self-supervised representation learning approach for visual reasoning and integrate it into a nonlinear program formulation for motion optimization to tackle sequential manipulation tasks. Such problems have usually been addressed by combined task and motion planning approaches, for which spatial relations and logical rules that rely on symbolic representations have to be predefined by the user. We propose to learn relational structures by leveraging visual perception to alleviate the resulting knowledge acquisition bottleneck. In particular, we learn constructing {\em scene-graphs}, that represent objects (``red box"), and their spatial relationships (`yellow cylinder on red box"). This representation allows us to plan high-level discrete decisions effectively using graph search algorithms. We integrate the visual reasoning module with a nonlinear optimization method for robot motion planning and verify its feasibility on the classic blocks-world domain. Our proposed framework successfully finds the sequence of actions and enables the robot to execute feasible motion plans to realize the given tasks.},    
    paperid  =  {465}    
}    

@InProceedings{julian20,    
    title  =  {Never Stop Learning: The Effectiveness of Fine-Tuning in Robotic Reinforcement Learning},    
    author  =  {Julian, Ryan and Swanson, Benjamin and Sukhatme, Gaurav and Levine, Sergey and Finn, Chelsea and Hausman, Karol},    
    pages  =  {2120--2136},     
    abstract  =  {One of the great promises of robot learning systems is that they will be able to learn from their mistakes and continuously adapt to ever-changing environments. Despite this potential, most of the robot learning systems today produce static policies that are not further adapted during deployment, because the algorithms which produce those policies are not designed for continual adaptation. We present an adaptation method, and empirical evidence that it supports a robot learning framework for continual adaption. We show that this very simple method-fine-tuning off-policy reinforcement learning using offline datasets--is robust to changes in background, object shape and appearance, lighting conditions, and robot morphology. We demonstrate how to adapt vision-based robotic manipulation policies to new variations using less than 0.2\% of the data necessary to learn the task from scratch. Furthermore, we demonstrate that this robustness holds in an episodic continual learning setting. We also show that pre-training via RL is essential: training from scratch or adapting from super vised ImageNet features are both unsuccessful with such small amounts of data. Our empirical conclusions are consistently supported by experiments on simulated manipulation tasks, and by 60 unique fine-tuning experiments on a real robotic grasping system pre-trained on 580,000 grasps. For video results and an overview of the methods and experiments in this study, see the project website at \url{https://ryanjulian.me/continual-fine-tuning}.},    
    paperid  =  {467}    
}    

@InProceedings{gillen20,    
    title  =  {Explicitly Encouraging Low Fractional Dimensional Trajectories Via Reinforcement Learning},    
    author  =  {Gillen, Sean and Byl, Katie},    
    pages  =  {2137--2147},     
    abstract  =  {A key limitation in using various modern methods of machine learning in developing feedback control policies is the lack of appropriate methodologies to analyze their long-term dynamics, in terms of making any sort of guarantees (even statistically) about robustness.  The central reasons for this are largely due to the so-called curse of dimensionality, combined with the black-box nature of the resulting control policies themselves. This paper aims at the first of these issues. Although the full state space of a system may be quite large in dimensionality, it is a common feature of most model-based control methods that the resulting closed-loop systems demonstrate dominant dynamics that are rapidly driven to some lower-dimensional sub-space within. In this work we argue that the dimensionality of this subspace is captured by tools from fractal geometry, namely various notions of a fractional dimension. We then show that the dimensionality of trajectories induced by model free reinforcement learning agents can be influenced adding a post processing function to the agents reward signal. We verify that the dimensionality reduction is robust to noise being added to the system and show that that the modified agents are more actually more robust to noise and push disturbances in general for the systems we examined.},    
    paperid  =  {476}    
}    

@InProceedings{cheng20,    
    title  =  {S3CNet: A Sparse Semantic Scene Completion Network for LiDAR Point Clouds},    
    author  =  {Cheng, Ran and Agia, Christopher and Ren, Yuan and Li, Xinhai and Bingbing, Liu},    
    pages  =  {2148--2161},     
    abstract  =  {With the increasing reliance of self-driving and similar robotic systems on robust 3D vision, the processing of LiDAR scans with deep convolutional neural networks has become a trend in academia and industry alike. Prior attempts on the challenging Semantic Scene Completion task - which entails the inference of dense 3D structure and associated semantic labels from ``sparse'' representations - have been, to a degree, successful in small indoor scenes when provided with dense point clouds or dense depth maps often fused with semantic segmentation maps from RGB images. However, the performance of these systems drop drastically when applied to large outdoor scenes characterized by dynamic and exponentially sparser conditions. Likewise, processing of the entire sparse volume becomes infeasible due to memory limitations and workarounds introduce computational inefficiency as practitioners are forced to divide the overall volume into multiple equal segments and infer on each individually, rendering real-time performance impossible. In this work, we formulate a method that subsumes the sparsity of large-scale environments and present S3CNet, a sparse convolution based neural network that predicts the semantically completed scene from a single, unified LiDAR point cloud. We show that our proposed method outperforms all counterparts on the 3D task, achieving state-of-the art results on the SemanticKITTI benchmark. Furthermore, we propose a 2D variant of S3CNet with a multi-view fusion strategy to complement our 3D network, providing robustness to occlusions and extreme sparsity in distant regions. We conduct experiments for the 2D semantic scene completion task and compare the results of our sparse 2D network against several leading LiDAR segmentation models adapted for bird's eye view segmentation on two open-source datasets.},    
    paperid  =  {478}    
}    

@InProceedings{singh20,    
    title  =  {Chaining Behaviors from Data with Model-Free Reinforcement Learning},    
    author  =  {Singh, Avi and Yu, Albert and Yang, Jonathan and Zhang, Jesse and Kumar, Aviral and Levine, Sergey},    
    pages  =  {2162--2177},     
    abstract  =  {Reinforcement learning has been applied to a wide variety of robotics problems, but most of such applications involve collecting data from scratch for each new task. Since the amount of robot data we can collect for any single task is limited by time and cost considerations, the learned behavior is typically narrow: the policy can only execute the task in a handful of scenarios that it was trained on. What if there was a way to incorporate a large amount of prior data, either from previously solved tasks or from unsupervised or undirected environment interaction, to extend and generalize learned behaviors? While most prior work on extending robotic skills using pre-collected data focuses on building explicit hierarchies or skill decompositions, we show in this paper that we can reuse prior data to extend new skills simply through model-free reinforcement learning via dynamic programming. We show that even when the prior data does not actually succeed at solving the new task, it can still be utilized for learning a better policy, by providing the agent with a broader understanding of the mechanics of its environment. We demonstrate the effectiveness of such an approach by chaining together several behaviors seen in prior datasets for solving a new task, with our hardest experimental setting involving composing four robotic skills in a row: picking, placing, drawer opening, and grasping, where a +1/0 sparse reward is provided only on task completion. We train our policies in an end-to-end fashion, mapping high-dimensional image observations to low-level robot control commands, and present results in both simulated and real world domains.},    
    paperid  =  {481}    
}    

@InProceedings{li20b,    
    title  =  {Differentiable Logic Layer for Rule Guided Trajectory Prediction},    
    author  =  {Li, Xiao and Rosman, Guy and Gilitschenski, Igor and DeCastro, Jonathan and Vasile, Cristian-Ioan and Karaman, Sertac and Rus, Daniela},    
    pages  =  {2178--2194},     
    abstract  =  {In this work, we propose a method for integration of temporal logic formulas into a neural network. Our main contribution is a new logic optimization layer that uses differentiable optimization on the formulas' robustness function. This allows incorporating traffic rules into deep learning based trajectory prediction approaches. In the forward pass, an initial prediction from a base predictor is used to initialize and guide the robustness optimization process. Backpropagation through the logic layer allows for simultaneously adjusting the parameters of the rules and the initial prediction network. The integration of a logic layer affords both improved predictions, as well as quantification rule satisfaction and violation during predictor execution. As such, it can serve as a parametric safety- envelope for black box behavior models. We demonstrate how integrating traffic rules improves the predictor performance using real traffic data from the NuScenes dataset.},    
    paperid  =  {482}    
}    

@InProceedings{gao20,    
    title  =  {Attentional Separation-and-Aggregation Network for Self-supervised Depth-Pose Learning in Dynamic Scenes},    
    author  =  {Gao, Feng and Yu, Jincheng and Shen, Hao and Wang, Yu and Yang, Huazhong},    
    pages  =  {2195--2205},     
    abstract  =  {Learning depth and ego-motion from unlabeled videos via self-supervision from epipolar projection can improve the robustness and accuracy of the 3D perception and localization of vision-based robots. However, the rigid projection computed by ego-motion cannot represent all scene points, such as points on moving objects, leading to false guidance in these regions. To address this problem, we propose an Attentional Separation-and-Aggregation Network (ASANet), which can learn to distinguish and extract the scene's static and dynamic characteristics via the attention mechanism. We further propose a novel MotionNet with an ASANet as the encoder, followed by two separate decoders, to estimate the camera's ego-motion and the scene's dynamic motion field. Then, we introduce an auto-selecting approach to detect the moving objects for dynamic-aware learning automatically. Empirical experiments demonstrate that our method can achieve the state-of-the-art performance on the KITTI benchmark.},    
    paperid  =  {487}    
}    

@InProceedings{gangwani20,    
    title  =  {Harnessing Distribution Ratio Estimators for Learning Agents with Quality and Diversity},    
    author  =  {Gangwani, Tanmay and Peng, Jian and Zhou, Yuan},    
    pages  =  {2206--2215},     
    abstract  =  {Quality-Diversity (QD) is a concept from Neuroevolution with some intriguing applications to Reinforcement Learning. It facilitates learning a population of agents where each member is optimized to simultaneously accumulate high task-returns and exhibit behavioral diversity compared to other members. In this paper, we build on a recent kernel-based method for training a QD policy ensemble with Stein variational gradient descent. With kernels based on $f$-divergence between the stationary distributions of policies, we convert the problem to that of efficient estimation of the ratio of these stationary distributions. We then study various distribution ratio estimators used previously for off-policy evaluation and imitation and re-purpose them to compute the gradients for policies in an ensemble such that the resultant population is diverse and of high-quality.},    
    paperid  =  {496}    
}    

@InProceedings{roh20,    
    title  =  {Multimodal Trajectory Prediction via Topological Invariance for Navigation at Uncontrolled Intersections},    
    author  =  {Roh, Junha and Mavrogiannis, Christoforos and Madan, Rishabh and Fox, Dieter and Srinivasa, Siddhartha},    
    pages  =  {2216--2227},     
    abstract  =  {We focus on decentralized navigation among multiple non-communicating rational agents at {\em uncontrolled} intersections, i.e., street intersections without traffic signs or signals. Avoiding collisions in such domains relies on the ability of agents to predict each others' intentions reliably, and react quickly. Multiagent trajectory prediction is NP-hard whereas the sample complexity of existing data-driven approaches limits their applicability. Our key insight is that the geometric structure of the intersection and the incentive of agents to move efficiently and avoid collisions (rationality) reduces the space of likely behaviors, effectively relaxing the problem of trajectory prediction. In this paper, we collapse the space of multiagent trajectories at an intersection into a set of modes representing different classes of multiagent behavior, formalized using a notion of topological invariance. Based on this formalism, we design Multiple Topologies Prediction (MTP), a data-driven trajectory-prediction mechanism that reconstructs trajectory representations of high-likelihood modes in multiagent intersection scenes. We show that MTP outperforms a state-of-the-art multimodal trajectory prediction baseline (MFP) in terms of prediction accuracy by 78.24\% on a challenging simulated dataset. Finally, we show that MTP enables our optimization-based planner, MTPnav, to achieve collision-free and time-efficient navigation across a variety of challenging intersection scenarios on the CARLA simulator.},    
    paperid  =  {497}    
}    

@InProceedings{puranic20,    
    title  =  {Learning from Demonstrations using Signal Temporal Logic},    
    author  =  {Puranic, Aniruddh and Deshmukh, Jyotirmoy and Nikolaidis, Stefanos},    
    pages  =  {2228--2242},     
    abstract  =  {Learning-from-demonstrations is an emerging paradigm to obtain effective robot control policies for complex tasks via reinforcement learning without the need to explicitly design reward functions. However, it is susceptible to imperfections in demonstrations and also raises concerns of safety and interpretability in the learned control policies. To address these issues, we use Signal Temporal Logic to evaluate and rank the quality of demonstrations. Temporal logic-based specifications allow us to create non-Markovian rewards, and also define interesting causal dependencies between tasks such as sequential task specifications. We validate our approach through experiments on discrete-world and OpenAI Gym environments, and show that our approach outperforms the state-of-the-art Maximum Causal Entropy Inverse Reinforcement Learning.},    
    paperid  =  {498}    
}    

@InProceedings{ivanovic20,    
    title  =  {MATS: An Interpretable Trajectory Forecasting Representation for Planning and Control},    
    author  =  {Ivanovic, Boris and Elhafsi, Amine and Rosman, Guy and Gaidon, Adrien and Pavone, Marco},    
    pages  =  {2243--2256},     
    abstract  =  {Reasoning about human motion is a core component of modern human-robot interactive systems. In particular, one of the main uses of behavior prediction in autonomous systems is to inform robot motion planning and control. However, a majority of planning and control algorithms reason about system dynamics rather than the predicted agent tracklets (i.e., ordered sets of waypoints) that are commonly output by trajectory forecasting methods, which can hinder their integration. Towards this end, we propose Mixtures of Affine Time-varying Systems (MATS) as an output representation for trajectory forecasting that is more amenable to downstream planning and control use. Our approach leverages successful ideas from probabilistic trajectory forecasting works to learn dynamical system representations that are well-studied in the planning and control literature. We integrate our predictions with a proposed multimodal planning methodology and demonstrate significant computational efficiency improvements on a large-scale autonomous driving dataset.},    
    paperid  =  {499}    
}    

@InProceedings{paigwar20,    
    title  =  {Robust Quadrupedal Locomotion on Sloped Terrains: A Linear Policy Approach},    
    author  =  {Paigwar, Kartik and Krishna, Lokesh and tirumala, sashank and khetan, naman and varma, aditya and joglekar, ashish and Bhatnagar, Shalabh and Ghosal, Ashitava and Amrutur, Bharadwaj and Kolathaya, Shishir},    
    pages  =  {2257--2267},     
    abstract  =  {In this paper, with a view toward fast deployment of locomotion gaits in low-cost hardware, we use a linear policy for realizing end-foot trajectories in the quadruped robot, Stoch 2. In particular, the parameters of the end-foot trajectories are shaped via a linear feedback policy that takes the torso orientation and the terrain slope as inputs. The corresponding desired joint angles are obtained via an inverse kinematics solver and tracked via a PID control law. Augmented Random Search, a model-free and a gradient-free learning algorithm is used to train this linear policy. Simulation results show that the resulting walking is robust to terrain slope variations and external pushes. This methodology is not only computationally light-weight but also uses minimal sensing and actuation capabilities in the robot, thereby justifying the approach.},    
    paperid  =  {501}    
}    

@InProceedings{clark20,    
    title  =  {Learning Predictive Models for Ergonomic Control of Prosthetic Devices},    
    author  =  {CLARK, GEOFFEY and Campbell, Joseph and Amor, Heni Ben},    
    pages  =  {2268--2278},     
    abstract  =  {We present Model-Predictive Interaction Primitives -- a robot learning framework for assistive motion in human-machine collaboration tasks which explicitly accounts for biomechanical impact on the human musculoskeletal system. First, we extend Interaction Primitives to enable predictive biomechanics: the prediction of future biomechanical states of a human partner conditioned on current observations and intended robot control signals. In turn, we leverage this capability within a model-predictive control strategy to identify the future ergonomic and biomechanical ramifications of potential robot actions. Optimal control trajectories are selected so as to minimize future physical impact on the human musculoskeletal system. We empirically demonstrate that our approach minimizes knee or muscle forces via generated control actions selected according to biomechanical cost functions. Experiments are performed in synthetic and real-world experiments involving powered prosthetic devices.},    
    paperid  =  {503}    
}    

@InProceedings{pfrommer20,    
    title  =  {ContactNets: Learning Discontinuous Contact Dynamics with Smooth, Implicit Representations},    
    author  =  {Pfrommer, Samuel and Halm, Mathew and Posa, Michael},    
    pages  =  {2279--2291},     
    abstract  =  {Common methods for learning robot dynamics assume motion is continuous, causing unrealistic model predictions for systems undergoing discontinuous impact and stiction behavior. In this work, we resolve this conflict with a smooth, implicit encoding of the structure inherent to contact-induced discontinuities. Our method, ContactNets, learns parameterizations of inter-body signed distance and contact-frame Jacobians, a representation that is compatible with many simulation, control, and planning environments for robotics. We furthermore circumvent the need to differentiate through stiff or non-smooth dynamics with a novel loss function inspired by the principles of complementarity and maximum dissipation. Our method can predict realistic impact, non-penetration, and stiction when trained on 60 seconds of real-world data.},    
    paperid  =  {506}    
}    

@InProceedings{sutanto20,    
    title  =  {Learning Equality Constraints for Motion Planning on Manifolds},    
    author  =  {Sutanto, Giovanni and Fern\'{a}ndez, Isabel Rayas and Englert, Peter and Ramachandran, Ragesh Kumar and Sukhatme, Gaurav},    
    pages  =  {2292--2305},     
    abstract  =  {Constrained robot motion planning is a widely used technique to solve complex robot tasks. We consider the problem of learning representations of constraints from demonstrations with a deep neural network, which we call Equality Constraint Manifold Neural Network (ECoMaNN). The key idea is to learn a level-set function of the constraint suitable for integration into a constrained sampling-based motion planner. Learning proceeds by aligning subspaces in the network with subspaces of the data. We combine both learned constraints and analytically described constraints into the planner and use a projection-based strategy to find valid points. We evaluate ECoMaNN on its representation capabilities of constraint manifolds, the impact of its individual loss terms, and the motions produced when incorporated into a planner.},    
    paperid  =  {515}    
}    

@InProceedings{voigt20,    
    title  =  {Multi-Level Structure vs. End-to-End-Learning in High-Performance Tactile Robotic Manipulation},    
    author  =  {Voigt, Florian and Johannsmeier, Lars and Haddadin, Sami},    
    pages  =  {2306--2316},     
    abstract  =  {In this paper we apply a multi-level structure to robotic manipulation learning. It consists of a hybrid dynamical system we denote skill and a parameter learning layer that leverages the underlying structure to simplify the problem at hand. For the learning layer we introduce a novel algorithm based on the idea of learning to partition the parameter solution space to quickly and efficiently find good and robust solutions to complex manipulation problems. In a benchmark comparison we show a significant performance increase compared with other black-box optimization algorithms such as HiREPS and particle swarm optimization. Furthermore, we validate and compare our approach on a very hard real-world manipulation problem, namely inserting a key into a lock, with state-of-the-art deep reinforcement learning.},    
    paperid  =  {516}    
}    

@InProceedings{lee20,    
    title  =  {Learning Arbitrary-Goal Fabric Folding with One Hour of Real Robot Experience},    
    author  =  {Lee, Robert and Ward, Daniel and Dasagi, Vibhavari and Cosgun, Akansel and Leitner, Juxi and Corke, Peter},    
    pages  =  {2317--2327},     
    abstract  =  {Manipulating deformable objects, such as fabric, is a long standing problem in robotics, with state estimation and control posing a significant challenge for traditional methods. In this paper, we show that it is possible to learn fabric folding skills in only an hour of self-supervised real robot experience, without human supervision or simulation. Our approach relies on fully convolutional networks and the manipulation of visual inputs to exploit learned features, allowing us to create an expressive goal-conditioned pick and place policy that can be trained efficiently with real world robot data only. Folding skills are learned with only a sparse reward function and thus do not require reward function engineering, merely an image of the goal configuration. We demonstrate our method on a set of towel-folding tasks, and show that our approach is able to discover sequential folding strategies, purely from trial-and-error. We achieve state-of-the-art results without the need for demonstrations or simulation, used in prior approaches.},    
    paperid  =  {518}    
}    

@InProceedings{chen20f,    
    title  =  {Robust Policies via Mid-Level Visual Representations: An Experimental Study in Manipulation and Navigation},    
    author  =  {Chen, Bryan and Sax, Alexander and Lewis, Francis and Armeni, Iro and Savarese, Silvio and Zamir, Amir and Malik, Jitendra and Pinto, Lerrel},    
    pages  =  {2328--2346},     
    abstract  =  {Vision-based robotics often factors the control loop into separate components for perception and control. Conventional perception components usually extract hand-engineered features from the visual input that are then used by the control component in an explicit manner. In contrast, recent advances in deep RL make it possible to learn these features end-to-end during training, but the final result is often brittle, fails unexpectedly under minuscule visual shifts, and comes with a high sample complexity cost. In this work, we study the effects of using mid-level visual representations asynchronously trained for traditional computer vision objectives as a generic and easy-to-decode perceptual state in an end-to-end RL framework. We show that the invariances provided by the mid-level representations aid generalization, improve sample complexity, and lead to a higher final performance. Compared to the alternative approaches for incorporating invariances, such as domain randomization, using asynchronously trained mid-level representations scale better to harder problems and larger domain shifts, and consequently, successfully trains policies for tasks where domain randomization or learning-from-scratch failed. Our experimental findings are reported on manipulation and navigation tasks using real robots as well as simulations.},    
    paperid  =  {526}    
}    

@InProceedings{kim20,    
    title  =  {Towards Autonomous Eye Surgery by Combining Deep Imitation Learning with Optimal Control},    
    author  =  {Kim, Ji Woong and Zhang, Peiyao and Gehlbach, Peter and Iordachita, Iulian and Kobilarov, Marin},    
    pages  =  {2347--2358},     
    abstract  =  {During retinal microsurgery, precise manipulation of the delicate retinal tissue is required for positive surgical outcome. However, accurate manipulation and navigation of surgical tools remain difficult due to a constrained workspace and the top-down view during the surgery, which limits the surgeon's ability to estimate depth. To alleviate such difficulty, we propose to automate the tool-navigation task by learning to predict relative goal position on the retinal surface from the current tool-tip position. Given an estimated target on the retina, we generate an optimal trajectory leading to the predicted goal while imposing safety-related physical constraints aimed to minimize tissue damage. As an extended task, we generate goal predictions to various points across the retina to localize eye geometry and further generate safe trajectories within the estimated confines. Through experiments in both simulation and with several eye phantoms, we demonstrate that our framework can permit navigation to various points on the retina within 0.089mm and 0.118mm in xy error which is less than the human's surgeon mean tremor at the tool-tip of 0.180mm. All safety constraints were fulfilled and the algorithm was robust to previously unseen eyes as well as unseen objects in the scene. Live video demonstration is available here: https://youtu.be/n5j5jCCelXk},    
    paperid  =  {528}    
}    

@InProceedings{chen20g,    
    title  =  {Deep Phase Correlation for End-to-End Heterogeneous Sensor Measurements Matching},    
    author  =  {Chen, Zexi and Xu, Xuecheng and Wang, Yue and Xiong, Rong},    
    pages  =  {2359--2375},     
    abstract  =  {The crucial step for localization is to match the current observation to the map. When the two sensor modalities are significantly different, matching becomes challenging. In this paper, we present an end-to-end deep phase correlation network (DPCN) to match heterogeneous sensor measurements. In DPCN, the primary component is a differentiable correlation-based estimator that back-propagates the pose error to learnable feature extractors, which addresses the problem that there are no direct common features for supervision. In addition, it eliminates the exhaustive evaluation in some previous methods, improving efficiency. With the interpretable modeling, the network is light-weighted and promising for better generalization. We evaluate the system on both the simulation data and Aero-Ground Dataset which consists of heterogeneous sensor images and aerial images acquired by satellites or aerial robots. The results show that our method is able to match the heterogeneous sensor measurements, outperforming the comparative traditional phase correlation and other learning-based methods. Code is available at https://github.com/jessychen1016/DPCN .},    
    paperid  =  {530}    
}    
