---
title: 'Social-VRNN: One-Shot Multi-modal Trajectory Prediction for Interacting Pedestrians'
abstract: Prediction of human motions is key for safe navigation of autonomous robots
  among humans. In cluttered environments, several motion hypotheses may exist for
  a pedestrian, due to its interactions with the environment and other pedestrians.
  Previous works for estimating multiple motion hypotheses require a large number
  of samples which limits their applicability in real-time motion planning. In this
  paper, we present a variational learning approach for interaction-aware and multi-modal
  trajectory prediction based on deep generative neural networks. Our approach can
  achieve faster convergence and requires significantly fewer samples comparing to
  state-of-the-art methods. Experimental results on real and simulation data show
  that our model can effectively learn to infer different trajectories. We compare
  our method with three baseline approaches and present performance results demonstrating
  that our generative model can achieve higher accuracy for trajectory prediction
  by producing diverse trajectories.
paperid: '182'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: brito21a
month: 0
tex_title: 'Social-VRNN: One-Shot Multi-modal Trajectory Prediction for Interacting
  Pedestrians'
firstpage: 862
lastpage: 872
page: 862-872
order: 862
cycles: false
bibtex_author: Brito, Bruno Ferreira de and Zhu, Hai and Pan, Wei and Alonso-Mora,
  Javier
author:
- given: Bruno Ferreira de
  family: Brito
- given: Hai
  family: Zhu
- given: Wei
  family: Pan
- given: Javier
  family: Alonso-Mora
date: 2021-10-04
address:
container-title: Proceedings of the 2020 Conference on Robot Learning
volume: '155'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 10
  - 4
pdf: https://proceedings.mlr.press/v155/brito21a/brito21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
