---
title: Positive-Unlabeled Reward Learning
abstract: Learning reward functions from data is a promising path towards achieving
  scalable Reinforcement Learning (RL) for robotics. However, a major challenge in
  training agents from learned reward models is that the agent can learn to exploit
  errors in the reward model to achieve high reward behaviors that do not correspond
  to the intended task. These reward delusions can lead to unintended and even dangerous
  behaviors. On the other hand, adversarial imitation learning frameworks (Ho et al.,
  2016) tend to suffer the opposite problem, where the discriminator learns to trivially
  distinguish agent and expert behavior, resulting in reward models that produce low
  reward signal regardless of the input state. In this paper, we connect these two
  classes of reward learning methods to positive-unlabeled (PU) learning, and show
  that by applying a large-scale PU learning algorithm to the reward learning problem,
  we can address both the reward under- and over-estimation problems simultaneously.
  Our approach drastically improves both GAIL and supervised reward learning, without
  any additional assumptions.
paperid: '45'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: xu21c
month: 0
tex_title: Positive-Unlabeled Reward Learning
firstpage: 205
lastpage: 219
page: 205-219
order: 205
cycles: false
bibtex_author: Xu, Danfei and Denil, Misha
author:
- given: Danfei
  family: Xu
- given: Misha
  family: Denil
date: 2021-10-04
address:
container-title: Proceedings of the 2020 Conference on Robot Learning
volume: '155'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 10
  - 4
pdf: https://proceedings.mlr.press/v155/xu21c/xu21c.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
