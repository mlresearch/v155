---
title: Learning rich touch representations through cross-modal self-supervision
abstract: The sense of touch is fundamental in several manipulation tasks, but rarely
  used in robot manipulation. In this work we tackle the problem of learning rich
  touch features from cross-modal self-supervision. We evaluate them identifying objects
  and their properties in a few-shot classification setting. Two new datasets are
  introduced using a simulated anthropomorphic robotic hand equipped with tactile
  sensors on both synthetic and daily life objects. Several self-supervised learning
  methods are benchmarked on these datasets, by evaluating few-shot classification
  on unseen objects and poses. Our experiments indicate that cross-modal self-supervision
  effectively improves touch representation, and in turn has great potential to enhance
  robot manipulation skills.
paperid: '320'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zambelli21a
month: 0
tex_title: Learning rich touch representations through cross-modal self-supervision
firstpage: 1415
lastpage: 1425
page: 1415-1425
order: 1415
cycles: false
bibtex_author: Zambelli, Martina and Aytar, Yusuf and Visin, Francesco and Zhou, Yuxiang
  and Hadsell, Raia
author:
- given: Martina
  family: Zambelli
- given: Yusuf
  family: Aytar
- given: Francesco
  family: Visin
- given: Yuxiang
  family: Zhou
- given: Raia
  family: Hadsell
date: 2021-10-04
address:
container-title: Proceedings of the 2020 Conference on Robot Learning
volume: '155'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 10
  - 4
pdf: https://proceedings.mlr.press/v155/zambelli21a/zambelli21a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v155/zambelli21a/zambelli21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
