---
title: 'PLAS: Latent Action Space for Offline Reinforcement Learning'
abstract: The goal of offline reinforcement learning is to learn a policy from a fixed
  dataset, without further interactions with the environment. This setting will be
  an increasingly more important paradigm for real-world applications of reinforcement
  learning such as robotics, in which data collection is slow and potentially dangerous.
  Existing off-policy algorithms have limited performance on static datasets due to
  extrapolation errors from out-of-distribution actions. This leads to the challenge
  of constraining the policy to select actions within the support of the dataset during
  training. We propose to simply learn the Policy in the Latent Action Space (PLAS)
  such that this requirement is naturally satisfied. We evaluate our method on continuous
  control benchmarks in simulation and a deformable object manipulation task with
  a physical robot. We demonstrate that our method provides competitive performance
  consistently across various continuous control tasks and different types of datasets,
  outperforming existing offline reinforcement learning methods with explicit constraints.
paperid: '386'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhou21b
month: 0
tex_title: 'PLAS: Latent Action Space for Offline Reinforcement Learning'
firstpage: 1719
lastpage: 1735
page: 1719-1735
order: 1719
cycles: false
bibtex_author: Zhou, Wenxuan and Bajracharya, Sujay and Held, David
author:
- given: Wenxuan
  family: Zhou
- given: Sujay
  family: Bajracharya
- given: David
  family: Held
date: 2021-10-04
address:
container-title: Proceedings of the 2020 Conference on Robot Learning
volume: '155'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 10
  - 4
pdf: https://proceedings.mlr.press/v155/zhou21b/zhou21b.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
